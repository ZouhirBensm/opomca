-- MySQL dump 10.13  Distrib 8.0.40, for macos13.7 (x86_64)
--
-- Host: localhost    Database: opomca
-- ------------------------------------------------------
-- Server version	8.0.32

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!50503 SET NAMES utf8mb4 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `all_data_per_page_fr`
--

DROP TABLE IF EXISTS `all_data_per_page_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `all_data_per_page_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `title` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `description` text CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  `page_url_identify` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `under_h1` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=15 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `all_data_per_page_fr`
--

LOCK TABLES `all_data_per_page_fr` WRITE;
/*!40000 ALTER TABLE `all_data_per_page_fr` DISABLE KEYS */;
INSERT INTO `all_data_per_page_fr` VALUES (1,'Opom Canada','Site web de blogs français','/','Opom'),(2,'Contact','Page de contact','/contact','##'),(3,'Organisation','Organisation Opom Canada','/organisation','Opom Canada'),(4,'À propos','Apprenez-en plus sur Pavage Gatineau Earnanswers, notre mission, nos valeurs et l\'équipe expérimentée dédiée à fournir des services de pavage, d\'asphalte et de béton de qualité supérieure à Gatineau, Québec.','/a-propos','Pavage, Asphalte & Béton résidentiel et commercial à Gatineau'),(5,'Drywall Kingston','Page web référencant les références vers https://drywallkingston.com','/seo/drywall-kingston','Liens'),(6,'Earnanswers','Page web référencant les références vers https://earnanswers.com','/seo/earnanswers','Liens'),(7,'Bidblock','Page web référencant les références vers https://bidblock.ca','/seo/bidblock','Liens'),(8,'Blog','##','/blog','Opom Canada'),(9,'Mention Légale','Découvrez les informations légales concernant notre site \'Pavage, l\'asphalte et le béton résidentiel et commercial à Gatineau\'. Conformez-vous aux règles et réglementations.','/tiroir1/mention-legale','Pavage, Asphalte & Béton résidentiel et commercial à Gatineau'),(10,'Politique de confidentialité','Votre vie privée est notre priorité. Lisez notre politique de confidentialité pour comprendre comment nous protégeons et utilisons vos données sur \'Pavage, l\'asphalte et le béton résidentiel et commercial à Gatineau\'.','/tiroir1/politique-de-confidentialite','Pavage, Asphalte & Béton résidentiel et commercial à Gatineau'),(11,'Review business idea! [Paving contractor]','##','/ral/thirty-two','Pavage Gatineau Asphalte Gatineau Earnanswers'),(12,'Review business idea! [Drywall contractor]','##','/ral/thirty-three',' Drywall Kingston Residential & Commercial Contractor'),(13,'Review business idea! [Web Agency]','##','/ral/thirty-four','Earnanswers Website Agency Gatineau'),(14,'Review business idea! [Paving contractor]','##','/ral/thirty-five','Pavage Laval Asphalte Laval Earnanswers');
/*!40000 ALTER TABLE `all_data_per_page_fr` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `blog_element_fr`
--

DROP TABLE IF EXISTS `blog_element_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `blog_element_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `title` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,
  `html_content` text CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  `datetime_published` datetime NOT NULL,
  `datetime_edited` datetime NOT NULL,
  `slug` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,
  `category_id` int DEFAULT NULL,
  `meta_description` text CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  `under_h1` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `lang` varchar(3) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `has_equ` bit(1) NOT NULL DEFAULT b'0',
  PRIMARY KEY (`id`),
  KEY `blog_element_category_FK` (`category_id`),
  CONSTRAINT `blog_element_fr_category_fr_FK` FOREIGN KEY (`category_id`) REFERENCES `category_fr` (`id`) ON DELETE RESTRICT ON UPDATE CASCADE
) ENGINE=InnoDB AUTO_INCREMENT=24 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `blog_element_fr`
--

LOCK TABLES `blog_element_fr` WRITE;
/*!40000 ALTER TABLE `blog_element_fr` DISABLE KEYS */;
INSERT INTO `blog_element_fr` VALUES (1,'Enhancing Your Space with Professional Drywall Services','<section>\n<h2>Why Professional Drywall Installation Matters</h2>\n<p>Investing in professional drywall installation is essential for ensuring a high-quality and long-lasting\n  finish. Expert installers bring a wealth of experience and precision to each project, guaranteeing that the\n  drywall is properly aligned, secured, and finished.</p>\n<h3>Residential Installation</h3>\n<p>For homeowners, professional drywall installation means a smooth and perfect finish for new constructions,\n  renovations, or additions. Properly installed drywall can greatly enhance the comfort and appearance of living\n  spaces.</p>\n<h3>Commercial Installation</h3>\n<p>Businesses benefit from professional installation by minimizing disruptions and ensuring that all work is\n  completed on schedule. High-quality drywall installation contributes to a professional and polished\n  environment that can impress clients and employees alike.</p>\n</section>\n<section>\n  <h2>The Importance of Drywall Repair and Patching</h2>\n  <p>Over time, drywall can suffer from various forms of damage, including cracks, holes, and water damage.\n    Addressing these issues promptly with professional repair services can prevent further deterioration and\n    maintain the structural integrity of your walls.</p>\n  <h3>Residential Repair</h3>\n  <p>Whether it\'s a small hole from an accident or extensive damage from a leak, professional repair services\n    ensure that your walls are restored to their original condition. This not only improves the appearance of your\n    home but also preserves its value.</p>\n  <h3>Commercial Repair</h3>\n  <p>In a business setting, damaged walls can create a poor impression on clients and customers. Professional\n    drywall repair services ensure that your commercial space remains in top condition, reflecting the quality and\n    professionalism of your business.</p>\n</section>\n<section>\n  <h2>Achieving the Perfect Finish with Drywall Texturing</h2>\n  <p>Drywall texturing is an art that can add significant character and depth to your interiors. From subtle\n    textures to bold designs, professional drywall texturing can transform the look and feel of any room.</p>\n  <h3>Residential Texturing</h3>\n  <p>Homeowners can choose from a variety of textures to match their personal style and home decor. Whether you\n    prefer a smooth, modern look or a more intricate design, professional texturing ensures a high-quality finish\n    that enhances your living space.</p>\n  <h3>Commercial Texturing</h3>\n  <p>For businesses, textured walls can create a distinctive and memorable interior that stands out to clients and\n    customers. Professional texturing services offer a range of options to suit any commercial setting, from\n    offices to retail spaces.</p>\n</section>\n<section>\n  <h2>Why Choose Drywall Kingston?</h2>\n  <ul>\n    <li>Skilled Professionals: Our team consists of highly trained and experienced drywall contractors who are\n      dedicated to delivering top-notch results.</li>\n    <li>Comprehensive Services: We offer a full range of drywall services, including installation, repair, and\n      texturing, to meet all your needs.</li>\n    <li>Customer Satisfaction: At Drywall Kingston, we prioritize customer satisfaction and strive to exceed\n      expectations on every project.</li>\n  </ul>\n  <p>For residents and businesses in Kingston, Ontario, seeking expert drywall services, <a\n      href=\"https://drywallkingston.com\" target=\"_blank\">Drywall Kingston</a> is the\n    trusted choice. Request a free quote, book a call, or visit our website at <a target=\"_blank\"\n      href=\"https://drywallkingston.com\">drywallkingston.com</a> to learn more about how we can help with your\n    drywall needs.\n  </p>\n\n  <address>\n    <p><strong>Contact Information:</strong></p>\n    <p>Address: 1786 Bath Rd #18, Kingston, ON K7M 4Y2, Canada</p>\n    <p>Phone: +1 (343) 477 1480</p>\n    <p>Website: <a href=\"https://drywallkingston.com\" target=\"_blank\">drywallkingston.com</a></p>\n  </address>\n</section>','2024-08-25 14:02:53','2024-08-25 14:02:53','enhancing-your-space-with-professional-drywall-services',1,'Discover why professional drywall installation, repair, and texturing are crucial for both residential and commercial projects. Drywall Kingston offers expert services, ensuring high-quality results that enhance the look and value of your space. Contact us today for a free quote!','Drywall Kingston','en',_binary '\0'),(13,'Qu\'est-ce que l\'asphalte ?','<p>L\'<a rel=\'noreferrer nofollow noopener\' href=\'https://levisiennes.com/les-10-etapes-de-fabrication-de-lasphalte/\' target=\'_blank\'>asphalte</a> est un matériau largement utilisé pour la construction de routes, d\'allées et de parkings en raison de sa durabilité, de sa flexibilité et de sa résistance aux intempéries. <br/><br/>\n\nCe blog vous présentera les composants de base de l\'<a rel=\'noreferrer nofollow noopener\' href=\'https://services.totalenergies.fr/professionnels/conseils/bitumes/quelles-differences-entre-bitume-asphalte-goudron\' target=\'_blank\'>asphalte</a> et de ses éléments scientifiques</p>\n\n\n<img src=\'/img/blog13/1.webp\' alt=\'Close-up image of a bike lane pavement, showing the smooth asphalt surface with a white bike lane marking painted clearly on the road.\' width=\'550\' height=\'550\'/>\n\n\n<h2>Mots utilisés dans le domaine de l\'asphalte</h2>\n\n<p>Avant de plonger dans les détails, voici quelques termes clés que vous rencontrerez dans le domaine de l\'asphalte :</p>\n\n\n<ul>\n<li><strong>Liant</strong> : substance collante qui maintient les matériaux ensemble, essentielle dans l\'<a rel=\'noreferrer nofollow noopener\' href=\'https://www.pavagemassie.ca/fr/guide-de-lasphalte-composition-et-elements/\' target=\'_blank\'>asphalte</a>.</li>\n<li><strong>Granulat</strong> : mélange de sable, de <a rel=\'noreferrer nofollow noopener\' href=\'https://fr.wikipedia.org/wiki/Gravier_(granulat)\' target=\'_blank\'>gravier</a> ou de pierre concassée qui forme la majeure partie de l\'asphalte.</li>\n<li><strong>Bitume</strong> : substance épaisse, noire et collante dérivée du pétrole brut, qui agit comme liant dans l\'<a rel=\'noreferrer nofollow noopener\' href=\'https://soumissionspavage.com/prix-entree-auto-asphalte-2024/\' target=\'_blank\'>asphalte</a>.</li>\n<li><strong>Viscosité</strong> : mesure de la résistance d\'un fluide à l\'écoulement, essentielle pour déterminer le niveau de fonctionnalité de l\'asphalte à différentes températures.</li>\n<li><strong>Mélange d\'asphalte</strong> : combinaison finale du granulat, de <a rel=\'noreferrer nofollow noopener\' href=\'https://www.inrs.fr/risques/bitume/de-quoi-parle-t-on.html\' target=\'_blank\'>bitume</a> et parfois d\'additifs, utilisée pour le pavage.</li>\n<li><strong>Stabilité</strong> : Dans le contexte de l\'asphalte, la stabilité fait référence à la capacité du mélange d\'asphalte à résister à des variations externes, telles que la pression exercée par le trafic routier, sans se déformer ni se décomposer. Elle indique dans quelle mesure le matériau peut conserver sa structure et son intégrité sous charge, en particulier lorsqu\'il est soumis à une utilisation intensive ou à des conditions météorologiques variables. Par exemple, un mélange d\'asphalte stable résiste à l\'orniérage (dépression dans les passages de roue), aux poussées (déplacements horizontaux) et aux fissures, ce qui garantit que les routes et les surfaces restent lisses et fonctionnelles pendant une période prolongée.</li>\n<li><strong>Intégrité</strong> : dans le contexte de l\'asphalte, l\'intégrité fait référence à la capacité de l\'asphalte à conserver sa structure, sa composition et sa fonction au fil du temps. Cela signifie que le matériau reste cohérent, sans fissures, nids-de-poule ou séparation entre ses composants, même après une exposition à des facteurs environnementaux tels que les intempéries, l\'humidité et les charges de trafic. L\'intégrité garantit que la surface de l\'asphalte reste durable et fonctionne comme prévu sans détérioration significative.</li>\n</ul>\n\n\n\n<h3>Structures d\'asphalte</h3>\n\n\n<h4>Structure du bitume</h4>\n\n<p>La <strong>structure du bitume</strong> fait référence à la composition interne et à la disposition moléculaire du bitume, qui est le liant utilisé dans l\'<a rel=\'noreferrer nofollow noopener\' href=\'https://www.larousse.fr/dictionnaires/francais/asphalte/5725\' target=\'_blank\'>asphalte</a>. Le <a rel=\'noreferrer nofollow noopener\' href=\'https://fr.mylivingbloom.com/allee-bordure/quelle-est-la-difference-entre-goudron-et-bitume-macadam-asphalte/\' target=\'_blank\'>bitume</a> est composé de divers hydrocarbures, notamment des saturés, des aromatiques, des résines et des asphaltènes. Ces composants confèrent au bitume ses propriétés clés, telles que l\'adhérence, la flexibilité et la viscosité. La structure du bitume affecte la façon dont il lie les matériaux de granulats agrégés et sa capacité à résister aux a l\'environnement, telles que les changements de température et d\'humidité. Le rapport de ces composants dans le bitume a un impact sur la qualité d\'asphalte.</p>\n\n<h4>Structure des agrégats</h4>\n\n<p>La <strong>structure des agrégats</strong> fait référence à la disposition et à la composition des granulats agrégés (pierre concassée, gravier, sable, etc.) dans le mélange d\'asphalte. La taille, la forme et la distribution des particules d\'agrégats jouent un rôle crucial dans la détermination de la résistance, de la stabilité et de la durabilité de l\'asphalte. La structure des agrégats aide à supporter les charges, à répartir uniformément la pression et à fournir une texture de surface pour la traction. Une structure de granulat bien conçue conduit à un meilleur compactage, à la réduction d\'endommagements et à une amélioration des performances à long terme de la chaussée en asphalte.</p>\n\n\n\n<h2>Principaux éléments de haut niveau dont est composé l\'asphalte</h2>\n\n<p>L\'asphalte est principalement composé de deux éléments principaux : le granulat et le liant. Ces composants fonctionnent ensemble pour créer le matériau qui se solidifie, mais visqueux lors du coulage que nous utilisons pour le pavage.</p>\n\n<h3>Granulat</h3>\n\n<p>Le granulat constitue la majeure partie de l\'asphalte et assure son intégrité structurelle. Il se compose de :</p>\n\n\n<ul>\n<li><strong>Pierre concassée</strong> : Les matériaux durs comme le granit ou le calcaire fournissent une base solide.</li>\n<li><strong>Sable</strong> : Comble les espaces entre les pierres et crée une finition lisse.</li>\n<li><strong>Gravier</strong> : Ajoute du volume et de la résistance au mélange.</li>\n</ul>\n\n\n<img src=\'/img/blog13/2.webp\' alt=\'Close-up image of aggregate used in asphalt, featuring various sizes of crushed stone, gravel, and sand particles in shades of gray, beige, and brown.\' width=\'550\' height=\'550\'/>\n\n\n\n\n<h2>Rôle des granulats dans la durabilité</h2>\n\n<p>La durabilité et la stabilité des granulats dépendent de leur granulométrie (la dimension des granulats) et de leur teneur en minéraux :</p>\n\n\n<ul>\n<li><strong>Granulométrie</strong> : les pierres plus grosses améliorent la stabilité, tandis que les granulats plus petits rend l\'asphalte plus lisse.</li>\n<li><strong>Teneur en minéraux</strong> : des minéraux comme le quartz ou le feldspath sont souvent présents, améliorant la dureté et la longévité du matériau.</li>\n</ul>\n\n\n<p>Ces aspects scientifiques garantissent que les granulats conservent leur résistance sous pression et dans les conditions environnementales changeantes.</p>\n\n\n\n<h2>Liant (bitume)</h2>\n\n<p>Le liant est ce qui maintient les granulats ensemble. Dans la plupart des cas, le liant est du bitume, un produit dérivé du pétrole brut. Il remplit plusieurs rôles essentiels :</p>\n\n\n<ul>\n<li>Adhérence aux granulats : le bitume lie le sable, le gravier et la pierre concassée en une masse solide.</li>\n<li>Imperméabilisation : il aide à prévenir les dégâts causés par l\'eau et l\'érosion.</li>\n<li>Résistance à la température : le bitume s\'adapte aux conditions météorologiques, devenant plus mou à la chaleur et plus rigide au froid.</li>\n</ul>\n\n\n\n\n<img src=\'/img/blog13/3.webp\' alt=\'Close-up image of bitumen, showing its dark, thick, and sticky texture with a shiny, black surface commonly used in road construction.\' width=\'550\' height=\'550\'/>\n\n\n\n\n\n\n\n<h2>Composition chimique du bitume</h2>\n\n<p>Le bitume est composé de différents hydrocarbures, chacun contribuant aux propriétés du liant :</p>\n\n\n<ul>\n<li>Les hydrocarbures saturés aident à maintenir la structure.</li>\n<li>Les hydrocarbures aromatiques augmentent la flexibilité.</li>\n<li>Les <strong>résines</strong> assurent l\'adhérence.</li>\n<li>Les <strong>asphaltènes</strong> : contribuent à la viscosité et à la dureté, affectant la façon dont le bitume réagit aux changements de température.</li>\n</ul>\n\n\n<p>Comprendre la composition chimique du bitume est essentiel pour savoir pourquoi l\'asphalte morphe, en particulier à des températures extrêmes.</p>\n\n\n\n<h2>Différents types de mélanges d\'asphalte</h2>\n\n<p>Le type de mélanges d\'asphalte qui est effectué en fonction du besoin, de la charge de trafic et du climat. Les types majeurs comprennent :</p>\n\n\n<ul>\n<li>L\'<strong><a rel=\'noreferrer nofollow noopener\' href=\'https://fr.mcasphalt.com/procedes/enrobes-chauds/\' target=\'_blank\'>asphalte enrobé à chaud</a></strong> : chauffé à haute température pour une flexibilité et une durabilité maximales, souvent utilisé pour les routes.</li>\n<li>L\'<strong><a rel=\'noreferrer nofollow noopener\' href=\'https://www.rfj.ch/rfj/Actualite/economie/L-asphalte-tiede-meilleur-pour-l-environnement-et-les-ouvriers.html\' target=\'_blank\'>asphalte enrobé tiède</a></strong> : produit à des températures plus basses pour réduire les émissions et économiser de l\'énergie.</li>\n<li><strong><a rel=\'noreferrer nofollow noopener\' href=\'https://fr.mcasphalt.com/procedes/enrobes-coules-a-froid/micro-surfacage/\' target=\'_blank\'>Asphalte mélangé à froid</a></strong> : utilisé pour les réparations temporaires comme les nids-de-poule.</li>\n</ul>\n\n\n<p>Chaque mélange est conçu pour des applications spécifiques en fonction des exigences du projet.</p>\n\n\n\n\n<h2>Le processus de production d\'asphalte</h2>\n\n<p>L\'asphalte est produit dans des usines spécialisées où les agrégats de granulats et le bitume sont mélangés. Le processus suit les étapes suivantes :</p>\n\n\n<ul>\n<li>Chauffage des agrégats de granulats : les pierres et le sable sont chauffés pour éliminer l\'humidité.</li>\n<li>Ajout du liant : le bitume est mélangé aux agrégats de granulats chauffés pour former un mélange cohérent.</li>\n<li>Refroidissement et stockage : L\'asphalte est légèrement refroidi et stocké avant d\'être transporté sur le site.</li>\n</ul>\n\n\n\n\n\n<h2>Utilisations et applications de l\'asphalte</h2>\n\n<p>La polyvalence de l\'asphalte le rend idéal pour diverses applications, notamment :</p>\n\n\n<ul>\n<li><strong>Routes et autoroutes</strong> : résiste au trafic intense et aux conditions météorologiques difficiles.</li>\n<li><strong>Allées</strong> : fournit une surface lisse et durable pour les propriétés résidentielles.</li>\n<li><strong>Parkings</strong> : offre une solution durable et économique pour les espaces commerciaux.</li>\n<li><strong>Terrains de sport</strong> : crée des surfaces lisses pour des activités comme le tennis ou le basket-ball.</li>\n</ul>\n\n\n<h2>Services de pavage de qualité à Gatineau - Pavage Gatineau</h2>\n\n<p>Vous recherchez des services de pavage fiables et de haute qualité à Gatineau ? Ne cherchez pas plus loin que <a href=\'https://pavagegatineau.com\' target=\'_blank\'>Pavage Gatineau</a> ! Spécialisés dans le pavage résidentiel et commercial, nous proposons des solutions d\'asphalte, de béton et de pavés autobloquants de premier ordre adaptées à vos besoins spécifiques. Avec un engagement envers l\'excellence, notre équipe s\'assure que chaque projet est terminé à temps et avec un savoir-faire supérieur. Qu\'il s\'agisse d\'allées, de stationnements ou de trottoirs, <a href=\'https://pavagegatineau.com\' target=\'_blank\'>Pavage Gatineau</a> garantit des résultats durables et professionnels. Visitez <a href=\'https://pavagegatineau.com\' target=\'_blank\'>pavagegatineau.com</a> dès aujourd\'hui pour obtenir un devis gratuit et rehausser vos espaces extérieurs avec des services de pavage haut de gamme !</p>\n\n\n\n<h2>Sites approuvés</h2>\n<ul>\n<li><a rel=\'noreferrer noopener\' href=\'https://www.eorvan.com/site-e-commerce\' target=\'_blank\'>Création de site e-commerce dans les Vosges par E-Orvan</a></li>\n<li><a rel=\'noreferrer noopener\' href=\'https://www.bagueenargent.com/produit/grosse-bague-argent-homme-2/\' target=\'_blank\'>Grosse Bague En Argent Homme</a></li>\n<li><a rel=\'noreferrer noopener\' href=\'https://digiseine.fr/\' target=\'_blank\'>Digiseine</a></li>\n</ul>','2024-08-25 17:41:28','2024-09-04 15:51:14','quest-ce-que-lasphalte',3,'Découvrez ce qu\'est l\'asphalte, ses principaux composants et comment il est utilisé dans la construction. Explorez des termes tels que bitume, agrégat et liant, et comprenez leur rôle dans la création de surfaces durables et résistantes aux intempéries.','Qu\'est-ce que l\'asphalte ?','fr',_binary '\0'),(14,'What is Law ?','<h2>My definition of law</h2>\n<p><a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/Law\' target=\'_blank\'>Law</a> is a list of best practice processes and permissible or unpermissible practices. <br/><br/> Law is then an identifiable series of codes or commands that gets executed by <a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/Law_enforcement\' target=\'_blank\'>law enforcers</a>. These enforcers manufacture abiding citizens through the threat of physical or monetary means. i.e. privation of liberty, expropriation of any form of property, and fines. Law is one of many instruments to dominate in a virtue signaling fashion. <strong>Law</strong> is also critical to maintaining civilization and is a form of legitimate violence.</p>\n   \n<img src=\'/img/blog14/1.webp\' alt=\'Abstract representation of law showing a blindfolded Lady Justice holding scales and a sword, overlaid with digital code. In the background are legal books, documents, and a gavel, symbolizing the intersection of law and technology.\' width=\'550\' height=\'550\'/>\n\n\n<h2>The Legacy realm</h2>\n<p>Sigmund Freud talks about this idea of sublimation. Where men\'s energy is directed towards deeds for the means of the end to procreate.<br/><br/>This concept is the cornerstone of human civilization because it pushes man to produce goods in exchange for resources. Therefore the world being a zero-sum game, <a rel=\'noreferrer nofollow noopener\' href=\'https://www.lsd.law/define/formal-law\' target=\'_blank\'>Formal law\'s</a> purpose in these modern times, is to extricate physical abundance from the masses and reinforce the position of those at the top of the hierarchy.<br/><br/>Because of men\'s competitive nature, our legacy <strong><a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/Legislature\' target=\'_blank\'>legislative structures</a></strong> have been split into 2: The Formal, and Informal.<br/><br/>Both are focused on the physical realm. The formal justice system because of factors like: semantics, social positioning and sophism is in reality designed to impede the laymen to challenge the status quo. Whereas the <a rel=\'noreferrer nofollow noopener\' href=\'https://www.cambridge.org/core/journals/journal-of-experimental-political-science/article/why-do-people-use-informal-justice-experimental-evidence-from-kosovo/2C63C1DB73840E7846C25210C8FBC777\' target=\'_blank\'>informal justice system</a> is designed to facilitate those well-positioned to rule the rules, that the layman is required to abide by.</p>\n\n\n<img src=\'/img/blog14/2.webp\' alt=\'Symbolic illustration of the evolution of law. On the left side, traditional law is represented by parchment, a quill, and scales of justice. On the right side, modern law is depicted with a digital tablet, coding symbols, and technological imagery, transitioning from an old courtroom setting to a virtual environment.\' width=\'550\' height=\'550\'/>\n\n\n<h2>Conclusion</h2>\n<p>In 2023, through principles, values and tools. One can propel their position to the top tier. Thanks to an unregulated and ever-changing new realm that showed up in the 1990s, called the internet, the average Joe can exploit this system to get ahead.</p>','2024-09-06 17:49:02','2024-09-06 17:49:02','what-is-law',4,'Explore the definition of law, its formal and informal mechanisms, and how the digital age presents new opportunities to rise in the social hierarchy through modern values, principles, and tools.','What is Law ?','en',_binary '\0'),(15,'Power of Software','<h2>Introduction: Wild West, Open Source Code</h2>\n\n\n<p>In this era, one, of many challenges, is to facillitate comunications between <a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/Computer\' target=\'_blank\'><strong>computers</strong></a> and humans. e.g. The fact there are so many <a rel=\'noreferrer nofollow noopener\' href=\'https://www.softr.io/no-code/no-code-tools\' target=\'_blank\'>no <strong>code</strong> tools</a> out there is a testimony to the fact.<br/><br/>Because of the complexity and sofistication of computers, some personality types disregard it\'s power and capabilities. Most of them happen to be in the field of politics.<br/><br/>This is positive, as the field stays un-hindered by empty suites with small pewees obstructing mass progress and societal shifts. The <a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/Software\' target=\'_blank\'>computer <strong>software</strong></a> industry is completly open for all to join, as long as one pocesses a computer and <strong>internet</strong> connection.</p>\n\n\n<img src=\'/img/blog15/1.webp\' alt=\'Abstract image illustrating the power of software with a computer displaying code and digital energy waves connecting different industry icons.\' width=\'550\' height=\'550\'/>\n\n\n\n\n<h2>Digital Law</h2>\n\n<p>Because of it\'s low barrier to entry, and the non existence of some type of buraucratie, the number of <a rel=\'noreferrer nofollow noopener\' href=\'https://www.herzing.edu/description/computer-programmer\' target=\'_blank\'>computer <strong>programmers</strong></a> is growing exponentially.<br/><br/>The Digital realm has become the series of commands we all subliminally abide by, by executing it. Without having identified it\'s power on our daily digital lives, it becomes difficult to make informed decisions.<br/><br/>Whether you are concsious of it or not, on a daily basis, you are executing thousands of lines of <a rel=\'noreferrer nofollow noopener\' href=\'https://news.mit.edu/2020/brain-reading-computer-code-1215\' target=\'_blank\'>code</a>, if not more, while working on you computer. The question becomes, is the code you are utilizing benefiting your bank balance or wasting you time on trivial maters?</p>\n\n<img src=\'/img/blog15/2.webp\' alt=\'Futuristic image showing the influence of software with a glowing tablet displaying code and various industry icons connected by digital lines.\' width=\'550\' height=\'550\'/>\n\n<h2>If It\'s free you are the product</h2>\n\n<p>The expression, \'If it is free, you are the product\', is a truthful one. This applies to the software you utilize</p>\n\n\n<h2>Choose the jurisdiction to live by, by land, choose the programmes to live by, by software</h2>\n\n<p>Ever since the creation of the <a rel=\'noreferrer nofollow noopener\' href=\'https://medium.com/modern-stack/how-much-computer-code-has-been-written-c8c03100f459\' target=\'_blank\'>computer code</a>, men utilize series of <strong>computer</strong> commands more and more.<br/><br/>Code has thus become <strong>jurisdiction</strong> in the <strong>digital realm</strong>, as it dictates what is, and what is not possible.<br/><br/> This <a rel=\'noreferrer nofollow noopener\' href=\'https://www.ecjlaw.com/ecj-blog/personal-jurisdiction-in-the-digital-age\' target=\'_blank\'>jusrisdiction</a> is either enforced by tech giants or small paid services 24/7. In 2023 a <strong>developer</strong> at Meta is now equivelent to a law maker in the 1970s.<br/><br/>To illustrate this, Instagram and Tiktok now made it a requirement for it\'s users to have 1,000 followers in order to display and properly render a link in one\'s bio.<br/><br/>So, one has to wonder, as some are being priviledged in the legacy <a rel=\'noreferrer nofollow noopener\' href=\'https://cyber.harvard.edu/property99/domain/Betsy.html\' target=\'_blank\'><strong>jurisdiction</strong></a>, how can one join the club of privileged players with this tech wild west jurisdiction 2.0.</p>\n\n\n<h2>Conclusion</h2>\n\n<p>To conclude, I invite you to what I consider the most powerful <a rel=\'noreferrer nofollow noopener\' href=\'https://www.techopedia.com/definition/4356/software\' target=\'_blank\'>software</a> to utilize today, and to completely disregard <a rel=\'noreferrer nofollow noopener\' href=\'https://contentrules.com/3-factors-to-consider-when-choosing-a-software-platform/\' target=\'_blank\'>software platforms</a> that do not benefit you. With the will of the All Mighty, you can make a shift in your life.</p>','2024-09-06 17:56:51','2024-09-06 17:56:51','power-of-software',5,'Explore the challenges of facilitating communication between humans and computers, the rise of open-source code, and how digital law is shaping the modern world. Discover how to navigate the tech industry\'s \'wild west\' to benefit your own success in the digital realm.','Power of Software','en',_binary '\0'),(16,'Explaining Funnels Through Proper Questions','<h2>Where are the eyeballs?</h2>\n\n<p>To a simple question, I\'ll give a simple answer.<br/><br/>The eyeballs are controlled by <a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/Big_Tech\' target=\'_blank\'><strong>big tech</strong> corporations</a> i.e. <strong>Google</strong>, <strong>Facebook</strong>, <strong>Instagram</strong>, <strong>TikTok</strong>, <strong>Twitter</strong>, etc.</p>\n\n\n<img src=\'/img/blog16/1.webp\' alt=\'Visual representation of a marketing funnel with stages like awareness, interest, decision, and action, with question marks symbolizing the importance of asking the right questions.\' width=\'550\' height=\'550\'/>\n\n<h2>What\'s the plan to make Money?</h2>\n\n<p>Your job is simply to divert those <strong>big platform</strong>\'s users to your URL, landing page, website or whatever you want to call it. This <a rel=\'noreferrer nofollow noopener\' href=\'https://www.salesforce.com/sales/what-is-lead-management/how-to-generate-leads/\' target=\'_blank\'>leads</a> to a tenet to <a rel=\'noreferrer nofollow noopener\' href=\'https://diggitymarketing.com/how-to-make-money-online/teenagers/\' target=\'_blank\'><strong>making money online</strong></a> i.e. Garner attention.<br/><br/>This is the main focus of the billions-of-dollar industry known as <a rel=\'noreferrer nofollow noopener\' href=\'https://mailchimp.com/resources/types-of-marketing/\' target=\'_blank\'><strong>Marketing</strong></a>. Now from this point, I can go into a trillion rabbit holes to explain how I would ideally proceed.<br/><br/>The path depends on your strengths and the platforms you decide to use, as they provide different functionality and cater to different needs, therefore I will not get into the weeds and simply give you directions to where to head out in your conquest to <a rel=\'noreferrer nofollow noopener\' href=\'https://moosend.com/blog/make-money-online/\' target=\'_blank\'>making money online</a>. As long as you stick to posting <a rel=\'noreferrer nofollow noopener\' href=\'https://www.constantcontact.com/blog/how-to-create-insanely-good-online-content/\' target=\'_blank\'><strong>high-quality content</strong></a>, you\'ll make it.</p>\n\n\n\n<img src=\'/img/blog16/2.webp\' alt=\'Diagram of a marketing funnel labeled with stages awareness, interest, decision, and action, with arrows and question marks showing progression.\' width=\'550\' height=\'550\'/>\n\n\n\n<h2>Realms of endeavors?</h2>\n\n<p>Simply put, if your a photographer, start posting professional photos on instagram. If you do copywrite, start a wordpress blog. If your a video editor, start a youtube channel. Try to align your skills to the platform you\'re trying to <strong>generate traffic</strong> from. Remember to focus on:<br/><br/>1. Generating exceptionally hight quality content, because of the <a rel=\'noreferrer nofollow noopener\' href=\'https://www.investopedia.com/terms/w/winner-takes-all-market.asp\' target=\'_blank\'>winner takes all effect</a><br/><br/>2. Make your content depending on <a rel=\'noreferrer nofollow noopener\' href=\'https://www.conductor.com/academy/glossary/search-volume/\' target=\'_blank\'>search query volume</a>. Start posting on topics with small <a rel=\'noreferrer nofollow noopener\' href=\'https://www.semrush.com/kb/683-what-is-search-volume-in-semrush\' target=\'_blank\'>search volumes</a>. Remember to have small expectations when starting, and target higher <strong>search volumes</strong> as you go.</p>','2024-09-06 18:03:06','2024-09-06 18:03:06','explaining-funnels-through-proper-questions',5,'Learn how to divert attention from major platforms like Google, Instagram, and TikTok to your website and start making money online. Focus on high-quality content and aligning your skills with the right platform for success.','Explaining Funnels Through Proper Questions','en',_binary '\0'),(17,'OkCupid Dataset Analysis for Machine Learning','<h2>Academic Machine Learning Project on Dating Data</h2>\n\n<p>\nThis blog article is part 1 of a series. The whole series constitutes a <a rel=\'noreferrer nofollow noopener\' href=\'https://uqo.ca/\' target=\'_blank\'>university</a> machine learning project.<br/><br/>\n\nHere are the other parts of the series:\n</p>\n\n<ul>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/exploring-okcupid-dataset-with-machine-learning-predicting-mating-success-for-straight-males\">Exploring OkCupid Dataset with Machine Learning: Predicting Mating Success for Straight Males</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/exploring-okcupid-dataset-with-stratified-sampling\">Exploring OkCupid Dataset with Stratified Sampling</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/discover-and-visualize-the-data-to-gain-insights-on-feature-vs-label-correlations-okcupid-dataset\">Discover and Visualize the Data to Gain Insights on Feature vs Label Correlations: OkCupid Dataset</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/okcupid-dataset-preparing-the-data-before-learning-algorithms\">OkCupid Dataset: Preparing the Data Before Learning Algorithms</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/comparing-model-performance-linear-regression-decision-tree-and-random-forest-for-predicting-mating-success\">Comparing Model Performance: Linear Regression, Decision Tree, and Random Forest for Predicting Mating Success</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/comparing-model-performance-on-training-data-vs-test-data\">Comparing Model Performance on Training Data vs. Test Data</a></li>\n</ul>\n\n\n<h2>Dataset Selection</h2>\n\n\n<p><a rel=\'noreferrer nofollow noopener\' href=\'https://www.kaggle.com/\' target=\'_blank\'>Kaggle</a> is a website that hosts a <a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/Data_set\' target=\'_blank\'>series of Datasets</a>.<br/><br/>\n\nFor my project, I found this <a rel=\'noreferrer nofollow noopener\' href=\'https://www.merriam-webster.com/dictionary/dataset\' target=\'_blank\'>dataset</a> that I will be using:\n</p>\n\n\n<ul>\n<li><b>Source</b>: Kaggle</li>\n<li><b>URL</b>: <a rel=\'noreferrer nofollow noopener\' href=\'https://www.kaggle.com/datasets/andrewmvd/okcupid-profiles\' target=\'_blank\'>https://www.kaggle.com/datasets/andrewmvd/okcupid-profiles</a></li>\n<li><b>Dataset Name</b>: <a rel=\'noreferrer nofollow noopener\' href=\'https://www.okcupid.com/\' target=\'_blank\'>OkCupid</a> Profiles</li>\n<li><b>Updated</b>: 4 years ago</li>\n<li><b>Made by</b>: Larxel</li>\n<li><b>More about the author</b>: <a rel=\'noreferrer nofollow noopener\' href=\'https://www.kaggle.com/andrewmvd\' target=\'_blank\'>https://www.kaggle.com/andrewmvd</a></li>\n<li><b>Description</b>: Explore dating app profiles looking for love</li>\n</ul>\n\n\n<p>\nI download the <a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/ZIP_(file_format)\' target=\'_blank\'>ZIP</a> from this URL:<br/><br/>\n\n<a rel=\'noreferrer nofollow noopener\' href=\'https://www.kaggle.com/datasets/andrewmvd/okcupid-profiles?resource=download\' target=\'_blank\'>https://www.kaggle.com/datasets/andrewmvd/okcupid-profiles?resource=download</a>\n<br/><br/>\n\nAfter I execute the code below:<br/><br/>\n\n./chapter2/data2/load2.py</p>\n\n\n<pre><code class=\"language-python\">import os\nimport zipfile\n\n# Define the path to the zip file and where to extract\ndating_zip_path = os.path.join(\"datasets\", \"dating\", \"dating.zip\")\nextract_path = os.path.join(\"datasets\", \"dating\")\n\n# Function to extract zip file\ndef extract_zip(zip_path, extract_to):\n    with zipfile.ZipFile(zip_path, \'r\') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(f\"Extracted to {extract_to}\")\n\n# Call the function to extract dating.zip\nextract_zip(dating_zip_path, extract_path)</code></pre>\n\n\n\n<pre><code class=\"language-python\">import os\nimport pandas as pd\n\n# Define the path to the dating dataset folder\nDATING_PATH = os.path.join(\"datasets\", \"dating\")\n\n# Function to load the dating CSV file\ndef load_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"okcupid_profiles.csv\")  # Adjust the filename if necessary\n    return pd.read_csv(csv_path)\n\n# Load the dating data\ndating_data = load_dating_data()\n\n# Display the first few rows of the dataset\nprint(dating_data.head())</code></pre>\n\n\n\n<p>\nThe code generates the <a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/Comma-separated_values\' target=\'_blank\'>CVS</a> in this directory:<br/><br/>\n</p>\n\n\n<pre><code class=\"language-python\">./datasets/dating/okcupid_profiles.csv</code></pre>\n\n\n\n<h2>OkCupid Dataset analysis</h2>\n\n\n<p>\nTo better understand our data, we will run a series of commands using <a rel=\'noreferrer nofollow noopener\' href=\'https://www.python.org/download/releases/3.0/\' target=\'_blank\'>Python 3</a> and a <a rel=\'noreferrer nofollow noopener\' href=\'https://jupyter.org/\' target=\'_blank\'>Jupiter notebook</a>.<br/><br/>\n\nBelow, we can see the first 3 rows of the data and the column names.\n</p>\n\n\n<img src=\'/img/blog17/p2.webp\' alt=\'Screenshot of an OKCupid dataset displaying user profiles with attributes such as age, status, body type, education, height, income, and various essays filled by users\' target=\'_blank\' width=\'1000\' height=\'auto\'/>\n\n\n\n\n<p>\nBelow are the columns with the number of valid entries for each. We can also see the data type that each column contains:\n</p>\n\n\n\n<img src=\"/img/blog17/4.webp\" alt=\"Pandas dataframe information showing columns and non-null counts for OKCupid dataset\" width=\"455\" height=\'757\'>\n\n\n\n\n\n<p>\nNote that \'status,\' for example, is a categorical attribute:<br/><br/>\n\nWe execute the code below to count the total number of entries that exist per category in the \'status\' column:\n</p>\n\n\n<img src=\"/img/blog17/5.webp\" alt=\"Status column value counts for OKCupid dataset showing distribution of relationship statuses\" width=\"383\" height=\'202\'>\n\n\n\n<p>\nTo retrieve statistics about columns that are of numerical type, we can execute the following:\n</p>\n \n\n<img src=\"/img/blog17/6.webp\" alt=\"Descriptive statistics for age, height, and income in OKCupid dataset\" width=\"620\" height=\'430\'>\n\n\n<p>\nSo, the units for the 3 columns of numerical type are:\n</p>\n\n\n<ul>\n<li><b>Age</b>: years</li>\n<li><b>Height</b>: inches</li>\n<li><b>Income</b>: US dollars</li>\n</ul>\n\n\n\n<p>\nNow we plot the histograms of the numeric columns (age, height and income (<a rel=\'noreferrer nofollow noopener\' href=\'https://vitalflux.com/what-are-features-in-machine-learning/\' target=\'_blank\'>features</a>)):<br/><br/>\n\nOn the <a rel=\'noreferrer nofollow noopener\' href=\'https://www.tutorialspoint.com/jupyter/index.htm\' target=\'_blank\'>notebook</a> run\n</p>\n\n\n\n<pre><code class=\"language-python\">%matplotlib inline\nimport matplotlib.pyplot as plt\ndating_data.hist(bins=50, figsize=(20,15))\nplt.show()</code></pre>\n\n\n<img src=\"/img/blog17/7.webp\" alt=\"Histogram plot of age distribution in the OKCupid dataset\" width=\"841\" height=\'631\'>\n\n\n<img src=\"/img/blog17/8.webp\" alt=\"Histogram plot of height distribution in the OKCupid dataset\" width=\"813\" height=\'609\'>\n\n\n<img src=\"/img/blog17/9.webp\" alt=\"Histogram plot of income distribution in the OKCupid dataset\" width=\"836\" height=\'657\'>\n\n\n<h2>Project proposal</h2>\n\n\n<p>\nIn this project, I am building a <a rel=\'noreferrer nofollow noopener\' href=\'https://www.ibm.com/topics/machine-learning\' target=\'_blank\'>machine learning</a> model to predict a combined label of relationship status and whether an individual has offspring. The goal is to understand how various demographic and physical characteristics contribute to relationship and parental status, specifically for males. The current features used for prediction, including ethnicity, body type, and height, are key personal attributes that may influence these outcomes, making them crucial to the model\'s accuracy and relevance to our study.<br/><br/>\n\nAdditional features, such as drinking habits, drug use, smoking status, religion, and languages spoken, could be incorporated to further enhance the model. By extending the feature set, the model could gain more insight into lifestyle and cultural factors that affect relationship dynamics and family structure. This would provide a more comprehensive understanding of the interactions between physical, demographic, and behavioural traits in predicting life choices related to family and relationships.<br/><br/>\n\n\nBelow the colour legend shows how the data will be dealt with:\n</p>\n\n\n\n<img src=\"/img/blog17/11.webp\" alt=\"Colored summary table for age, sex, orientation, drinks, and other attributes for OKCupid profiles\" width=\"834\" height=\'245\'>\n\n\n\n<ul>\n<li><b>Green</b>: will be separated based on category</li>\n<li><b>Blue</b>: labels</li>\n<li><b>Red</b>: important features</li>\n<li><b>Orange</b>: features of medium importance</li>\n<li><b>Yellow</b>: features of low importance</li>\n</ul>\n\n\n\n<h2>Information about other features</h2>\n\n\n<ul>\n<li><b>essay0</b> - My self summary</li>\n<li><b>essay1</b> - What I\'m doing with my life</li>\n<li><b>essay2</b> - I\'m really good at</li>\n<li><b>essay3</b> - The first thing people usually notice about me</li>\n<li><b>essay4</b> - Favorite books, movies, shows, music, and food</li>\n<li><b>essay5</b> - The six things I could never do without</li>\n<li><b>essay6</b> - I spend a lot of time thinking about</li>\n<li><b>essay7</b> - On a typical Friday night, I am</li>\n<li><b>essay8</b> - The most private thing I am willing to admit</li>\n<li><b>essay9</b> - You should message me if…</li>\n</ul>','2024-09-30 14:17:26','2024-11-23 18:00:40','okcupid-dataset-analysis-for-machine-learning',6,'Explore an in-depth analysis of the OkCupid dataset using machine learning. This blog series examines how ethnicity, height, and body type impact relationship success. Part 1 introduces the dataset selection, initial Python analysis, and project proposal for predicting relationship and parental status.','How Ethnicity, Height, and Body Type Effect Mating Success?','en',_binary '\0'),(18,'Exploring OkCupid Dataset with Machine Learning: Predicting Mating Success for Straight Males','<h2>Re-adjusting the project proposal</h2>\n\n\n<p>From the previous blog article titled <a href=\'/blog/artificial-intelligence/blog-posting/okcupid-dataset-analysis-for-machine-learning\' target=\'_blank\'><strong><a rel=\'noreferrer nofollow noopener\' href=\'https://www.okcupid.com/\' target=\'_blank\'> OkCupid </a></strong> <strong><a rel=\'noreferrer nofollow noopener\' href=\'https://www.markovml.com/blog/analyze-datasets\' target=\'_blank\'>Dataset Analysis</a></strong> for <strong>Machine Learning</strong></a>, we have decided to edit the choice of our features.<br/><br/>\n\nThis project aims to create a <strong><a rel=\'noreferrer nofollow noopener\' href=\'https://www.techtarget.com/searchenterpriseai/definition/predictive-modeling\' target=\'_blank\'>predictive model</a></strong> by focusing exclusively on male entries from the <strong><a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/Data_set\' target=\'_blank\'> dataset </a></strong>.<br/><br/>\n\nThe <a rel=\'noreferrer nofollow noopener\' href=\'https://toloka.ai/blog/machine-learning-labels-and-features/\' target=\'_blank\'>target label</a> will combine the \"offspring\" and \"status\" columns, which together capture <strong><a rel=\'noreferrer nofollow noopener\' href=\'https://www.sciencedirect.com/topics/biochemistry-genetics-and-molecular-biology/mating-success\' target=\'_blank\'>mating success</a></strong> for <strong>males</strong>.<br/><br/>\n\nFor the model\'s primary <a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/Feature_(machine_learning)\' target=\'_blank\'>features</a>, we will focus on \"body_type,\" \"ethnicity,\" and \"height\" as they may provide significant insights.<br/><br/>\n\nAdditionally, secondary <a rel=\'noreferrer nofollow noopener\' href=\'https://nebius.com/blog/posts/features-in-ml\' target=\'_blank\'>features</a> such as \"drinks,\" \"drugs,\" \"smokes,\" and \"religion\" will be incorporated to capture lifestyle and personal habits.<br/><br/>\n\nFinally, \"income\" may also be included as a tertiary feature to explore its potential influence, though it will have a lower priority in the analysis.<br/><br/>\n\nThe aim is to carefully prepare and optimize this feature set to train a robust <strong>predictive model</strong>.</p>\n\n<img src=\"/img/blog18/1.webp\" alt=\"A screenshot of a sample dataset from the OkCupid dataset showing various features like body type, diet, drinks, drugs, age, status, sex, orientation, education, ethnicity, height, income, pets, religion, location, offspring, and multiple essay sections.\" width=\"486\" height=\'1152\'>\n\n<ul>\n<li>Blue: labels</li>\n<li>Red: primary features</li>\n<li>Orange: secondary features</li>\n<li>Yellow: tertiary features</li>\n</ul>\n\n\n\n\n<h2>Adjusting the data</h2>\n\n\n<h3>Keep only sex=male, and orientation=straight entries in the dataset</h3>\n\n<p>The OkCupid dataset contains various entries with different genders and orientations. Since our project aims to focus exclusively on mating success for straight males, the first step is to explore the available entries for the sex and orientation columns and filter out only those labelled as male and straight.<br/><br/>\n\nWe will then store the filtered data in a new DataFrame and save it as a CSV file for further analysis.<br/><br/>\n\nThe following Python code demonstrates how to inspect the available values in the sex and orientation columns and filter the data to keep only entries where the sex is \"male\" and the orientation is \"straight.\"<br/><br/>\n\n./chapter2/data2/data_manipulations2/step1.py</p>\n\n\n<pre><code class=\"language-python\">dating_data[\"sex\"].value_counts()\nsex\nm    35829\nf    24117\nName: count, dtype: int64</code></pre>\n\n<pre><code class=\"language-python\">dating_data[\"orientation\"].value_counts()\norientation\nstraight    51606\ngay          5573\nbisexual     2767\nName: count, dtype: int64</code></pre>\n\n\n<pre><code class=\"language-python\">import os\nimport pandas as pd\n\n# Define the path to the dating dataset folder\nDATING_PATH = os.path.join(\"datasets\", \"dating\")\n\n# Function to load the dating CSV file\ndef load_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"okcupid_profiles.csv\")  # Adjust the filename if necessary\n    return pd.read_csv(csv_path)\n\n# Load the dating data\ndating_data = load_dating_data()\n\n# Filter the dataset to keep only male entries\nmale_data = dating_data[dating_data[\'sex\'] == \'m\']\n\n# Further filter the dataset to keep only entries with orientation \'straight.\'\nmale_straight_data = male_data[male_data[\'orientation\'] == \'straight\']\n\n# Check the shape of the new filtered DataFrame to confirm the number of entries\nprint(f\"Number of male and straight entries: {male_straight_data.shape[0]}\")\n\n# Save the filtered DataFrame to a new CSV file\noutput_path = \"/Users/Zouhir/Documents/UQO_AUT2024/INF6333_code/book_aurelien_geron/chapter2/datasets/dating/step1_male_straight_data.csv\"\nmale_straight_data.to_csv(output_path, index=False)\n\nprint(f\"Filtered male and straight entries saved to: {output_path}\")\n\n\nNumber of male and straight entries: 31073\nFiltered male and straight entries saved to: /Users/Zouhir/Documents/UQO_AUT2024/INF6333_code/book_aurelien_geron/chapter2/datasets/dating/step1_male_straight_data.csv</code></pre>\n\n\n<h2>Create a Test Set</h2>\n\n<p>When working with machine learning, we typically divide the dataset into train and test sets:</p>\n\n\n<ul>\n<li><strong><a rel=\'noreferrer nofollow noopener\' href=\'https://developers.google.com/machine-learning/crash-course/overfitting/dividing-datasets\' target=\'_blank\'>Train Set</a></strong>: Used to teach the model patterns in the data.</li>\n<li><strong><a rel=\'noreferrer nofollow noopener\' href=\'https://www.obviously.ai/post/the-difference-between-training-data-vs-test-data-in-machine-learning\' target=\'_blank\'>Test Set</a></strong>: Used to evaluate the model on unseen data to estimate how well it generalizes.</li>\n</ul>\n\n\n<p>However, datasets evolve. New entries might be added (e.g., more data collected), or some rows might be removed (e.g., duplicates or incorrect entries). If you don\'t have a stable way to split the data, your train and test sets could change between runs, leading to inconsistent results.<br/><br/>\n\nIf your split is based on persistent IDs, deleting entries from the training set will not change the composition of the test set. The remaining test entries will stay the same as they were, ensuring no data leakage or unintended reassignment.<br/><br/>\n\nIf new entries are added to the dataset (with their persistent IDs), they will be assigned to either the train or test set based on the same split logic. This ensures that the test set remains consistent, with only new data getting appropriately assigned.<br/><br/>\n\nOne of the key benefits of using persistent IDs is that it guarantees consistent splits across dataset modifications.<br/><br/>\n\n\nLet\'s generate persistent IDs for our dataset:<br/><br/>\n\n\n./chapter2/data2/data_manipulations2/step2.py</p>\n\n\n<pre><code class=\"language-python\">import os\nimport pandas as pd\nimport uuid  # To generate unique persistent IDs\n\n# Define the path to the dataset folder\nDATING_PATH = os.path.join(\"datasets\", \"dating\")\n\n# Function to load the dataset (male-only, straight)\ndef load_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"step1_male_straight_data.csv\")  # Ensure correct file extension\n    return pd.read_csv(csv_path)\n\n# Load the male-straight-only dataset\nmale_data = load_dating_data()\n\n# Create a persistent ID for each row if it doesn\'t already exist\nif \'persistent_id\' not in male_data.columns:\n    male_data[\'persistent_id\'] = [str(uuid.uuid4()) for _ in range(len(male_data))]\n    print(\"Persistent IDs generated and assigned to all entries.\")\n\n# Save the updated dataset with the new persistent IDs\noutput_path = os.path.join(DATING_PATH, \"male_straight_data_with_persistent_id.csv\")\nmale_data.to_csv(output_path, index=False)\n\nprint(f\"Dataset with persistent IDs saved to: {output_path}\")\n\nPersistent IDs are generated and assigned to all entries.\nDataset with persistent IDs saved to: datasets/dating/male_straight_data_with_persistent_id.csv</code></pre>\n\n\n\n\n\n<p>Now that each entry has its respective and unique persistent ID, we can go ahead and execute the code below to split the data based on a 20% and 80% ratio for testing and training data, respectfully:<br/><br/>\n\n./chapter2/data2/train_test_data/script3.py</p>\n\n\n\n\n<pre><code class=\"language-python\">from zlib import crc32  # Import CRC32 hash function for consistency\nimport numpy as np\nimport pandas as pd\nimport os\n\n# Define the path to the dataset folder\nDATING_PATH = os.path.join(\"datasets\", \"dating\")\n\n# Function to load the dataset (male-only, straight, with persistent IDs)\ndef load_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"male_straight_data_with_persistent_id.csv\")\n    return pd.read_csv(csv_path)\n\n# Load the male-only data\nmale_straight_data_with_persistent_id = load_dating_data()\n\n# Function to check if an identifier belongs in the test set based on CRC32 hash\ndef test_set_check(identifier, test_ratio):\n    # Calculate hash value and assign based on the test ratio\n    return crc32(identifier.encode()) & 0xffffffff < test_ratio * 2**32\n\n# Function to split the dataset using the \'persistent_id\' column\ndef split_train_test_by_id(data, test_ratio, id_column):\n    ids = data[id_column].astype(str)  # Convert IDs to string if necessary\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))  # Apply hash check\n    return data.loc[~in_test_set], data.loc[in_test_set]  # Return train and test sets\n\n# Split the male-only data into 80% train and 20% test sets using \'persistent_id\'\ntrain_set, test_set = split_train_test_by_id(male_straight_data_with_persistent_id, 0.2, \"persistent_id\")\n\n# Display the sizes of the train and test sets\nprint(f\"Training set size: {train_set.shape[0]}\")\nprint(f\"Test set size: {test_set.shape[0]}\")\n\n# Save the train and test sets as CSV files\ntrain_set.to_csv(os.path.join(DATING_PATH, \"male_straight_data_with_persistent_id_train_set.csv\"), index=False)\ntest_set.to_csv(os.path.join(DATING_PATH, \"male_straight_data_with_persistent_id_test_set.csv\"), index=False)\n\nprint(\"Train and test sets have been successfully saved.\")</code></pre>\n\n\n\n\n<h2>Conclusion</h2>\n\n\n<p>Through this deep dive into the <strong>OkCupid <a rel=\'noreferrer nofollow noopener\' href=\'https://developers.google.com/machine-learning/crash-course/overfitting\' target=\'_blank\'>dataset</a></strong>, we\'ve laid the groundwork for predicting mating success for straight males by carefully refining our <a rel=\'noreferrer nofollow noopener\' href=\'https://www.hopsworks.ai/dictionary/feature\' target=\'_blank\'>feature</a> set and using persistent IDs for stable train-test splits. We\'ve explored essential concepts, from filtering relevant data to calculating <a rel=\'noreferrer nofollow noopener\' href=\'https://he3.app/blogs/understanding-the-crc32-hash-a-comprehensive-guide/\' target=\'_blank\'>CRC32 hashes</a>, and applied these methods to ensure consistent results across multiple runs.<br/><br/>\n\nOur next steps will focus on applying <a rel=\'noreferrer nofollow noopener\' href=\'https://www.simplilearn.com/10-algorithms-machine-learning-engineers-need-to-know-article\' target=\'_blank\'>machine learning models</a> to uncover patterns between body type, ethnicity, height, and personal habits, aiming to predict which factors play the most significant role in <a rel=\'noreferrer nofollow noopener\' href=\'https://mjemmanuella.medium.com/the-complete-guide-to-achieving-success-in-modern-dating-6367141a1f1a\' target=\'_blank\'>dating success</a>.<br/><br/>\n\nTo get the full picture of this project, I recommend having a read of the following blog posts:</p>\n\n\n<ul>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/okcupid-dataset-analysis-for-machine-learning\">OkCupid Dataset Analysis for Machine Learning</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/exploring-okcupid-dataset-with-stratified-sampling\">Exploring OkCupid Dataset with Stratified Sampling</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/discover-and-visualize-the-data-to-gain-insights-on-feature-vs-label-correlations-okcupid-dataset\">Discover and Visualize the Data to Gain Insights on Feature vs Label Correlations: OkCupid Dataset</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/okcupid-dataset-preparing-the-data-before-learning-algorithms\">OkCupid Dataset: Preparing the Data Before Learning Algorithms</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/comparing-model-performance-linear-regression-decision-tree-and-random-forest-for-predicting-mating-success\">Comparing Model Performance: Linear Regression, Decision Tree, and Random Forest for Predicting Mating Success</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/comparing-model-performance-on-training-data-vs-test-data\">Comparing Model Performance on Training Data vs. Test Data</a></li>\n</ul>\n\n\n\n<h2>Annexe</h2>\n<h3>Explanation of the splitting</h3>\n\n\n\n<p>Let\'s use an example \n\n\n\n<a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/Universally_unique_identifier\' target=\'_blank\'>UUID</a>:<br/><br/>\n\nd8ea2885-a97e-4400-bc52-cf093c343e10<br/><br/>\n\nThis UUID is in <a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/Hexadecimal\' target=\'_blank\'>hexadecimal format (base-16)</a>. Each two-hex character represents one <a rel=\'noreferrer nofollow noopener\' href=\'https://www.techtarget.com/searchstorage/definition/byte\' target=\'_blank\'>byte</a> (or eight bits). Let\'s convert these values.</p>\n\n\n\n\n<h4>Hexadecimal to Decimal Conversion</h4>\n\n<p>Hexadecimal values use base-16, meaning each digit can be from 0 to F (where A = 10, B = 11, ..., F = 15).<br/><br/>\n\nd in hex = 13 (<a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/Decimal\' target=\'_blank\'>decimal</a>)<br/>\n8 in hex = 8 (decimal)</p>\n\n\n\n\n\n<h5>Expand Using Positional Values:</h5>\n\n<p>Each position in hexadecimal corresponds to a power of 16.<br/><br/>\n\n\n\\[ d8_{16} = (13 \\times 16^1) + (8 \\times 16^0) \\]\n\\[ d8_{16} = (13 \\times 16) + (8 \\times 1) = 208 + 8 = 216 \\]\n\nThus, d8 in hex equals 216 in decimal.</p>\n\n\n\n\n<h4>Converting the Entire UUID to Bytes</h4>\n\n<p>Let\'s now convert all parts of the UUID into decimal values and byte (binary) values.</p>\n\n\n<img src=\"/img/blog18/2.webp\" alt=\"A table showing the conversion of hexadecimal pairs to decimal, binary, and byte string equivalents.\" width=\"904\" height=\'703\'>\n\n\n\n<h4>Resulting Byte String</h4>\n\n<p>Once converted, the UUID becomes the following 16-<a rel=\'noreferrer nofollow noopener\' href=\'https://www.freecodecamp.org/news/python-bytes-to-string-how-to-convert-a-bytestring/\' target=\'_blank\'>byte string</a>:<br/><br/>\n\nb\'\\xd8\\xea(\\x85\\xa9~D\\x00\\xbcR\\xcf\\t<C>\\x10\'<br/><br/>\n\nThis byte string is the format needed for the <a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/Cyclic_redundancy_check\' target=\'_blank\'>CRC32 hashing function</a>.</p>\n\n\n\n\n<h4>CRC32 Hashing Process</h4>\n\n<p>Now, let\'s compute the CRC32 hash of the byte string step-by-step.<br/><br/>\n\n\n\n\n<h5>What is CRC32?</h5>\n\n<p>CRC32 stands for Cyclic Redundancy Check. A polynomial division algorithm produces a 32-bit integer hash from input data (in our case, the UUID byte string).<br/><br/>\n\nThe output is a 32-bit hash, ranging from 0 to 2^32 - 1 (4,294,967,295).</p>\n\n\n<pre><code class=\"language-python\">from zlib import crc32\n\n# Compute CRC32 hash of the byte string\nbyte_string = b\'\\xd8\\xea(\\x85\\xa9~D\\x00\\xbcR\\xcf\\t<C>\\x10\'\nhash_value = crc32(byte_string)\nprint(hash_value)  # Example output: 2441647152</code></pre>\n\n\n\n\n\n\n\n<h4>Ensure 32-bit Unsigned Integer with & 0xffffffff</h4>\n\n\n\n\n<p>When CRC32 computes a hash, it may return a signed 32-bit integer (which can be negative). However, we want to ensure the hash is treated as an unsigned 32-bit integer.</p>\n\n<h5>Bitwise AND Operation:</h5>\n\n<p>unsigned_hash = hash_value & 0xffffffff<br/>\n0xffffffff is the largest 32-bit unsigned integer:<br/><br/>\n\n\n0xffffffff = 4,294,967,295 (decimal)<br/><br/>\n\nThe & (<a rel=\'noreferrer nofollow noopener\' href=\'https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Bitwise_AND\' target=\'_blank\'>bitwise AND</a>) operation keeps only the lower 32 bits of the hash, ensuring it\'s treated as unsigned.</p>\n\n<h5>In our example:</h5>\n\n\n<p>2441647152 & 0xffffffff = 2441647152<br/><br/>\n\nSince the hash is already positive, this operation doesn\'t change its value.</p>\n\n\n\n\n\n\n\n<h4>Compare the Hash with the Test Ratio Threshold</h4>\n\n<p>Let\'s assume the test ratio is 0.2 (20%).</p>\n\n<h5>Calculate the Threshold:</h5>\n\n<p>\\[ \\text{Threshold} = 0.2 \\times 2^{32} = 0.2 \\times 4,294,967,296 = 858,993,459.2 \\]\n\n<br/><br/>\n\nWe compare the hash value with the threshold:<br/><br/>\n\n2441647152 < 858993459  # False<br/><br/>\n\nSince 2441647152 is greater than the threshold, this entry does not belong to the test set. It will be assigned to the training set.</p>','2024-10-17 15:59:23','2024-11-23 18:00:40','exploring-okcupid-dataset-with-machine-learning-predicting-mating-success-for-straight-males',6,'Discover how we refine the OkCupid dataset by filtering and re-adjusting key features for predictive modeling. Learn about persistent IDs, CRC32 hashing, and train-test splitting to ensure data consistency in machine learning.','From Data Preparation to Feature Engineering for Accurate Predictions','en',_binary ''),(19,'Exploring OkCupid Dataset with Stratified Sampling','<h2>Challenges with Random Sampling and Feature Imbalance</h2>\n\n<p>In the preceding blog titled <a href=\'/blog/artificial-intelligence/blog-posting/exploring-okcupid-dataset-with-machine-learning-predicting-mating-success-for-straight-males\' target=\'_blank\'>Exploring OkCupid Dataset with <strong>Machine Learning</strong>: Predicting Mating Success for Straight Males</a>, we split the <a rel=\'noreferrer nofollow noopener\' href=\'https://labelyourdata.com/articles/what-is-dataset-in-machine-learning\' target=\'_blank\'>dataset</a> to retrieve the test and train data using a <strong>random sampling method</strong>.<br/><br/>\n\nHowever, the <a rel=\'noreferrer nofollow noopener\' href=\'https://www.ibm.com/docs/en/ias?topic=sampling-usage\' target=\'_blank\'>random sampling method</a> we used in the previous blog does not maintain the ratio of a specific feature in the dataset, which is a significant issue.<br/><br/>\n\nFor example, if 60% of entries in our dataset have an ethnicity feature set to \'Middle Eastern,\' we want that same proportion represented in the test set.<br/><br/>\n\nOtherwise, the sampling is skewed.</p>\n\n\n\n\n\n<h2>Exploring Possible Ethnicity Values for Classification</h2>\n\n<p>We will examine all the possible values that the features can take to facilitate the classification of our total dataset entries based on ethnicity.</p>\n\n\n\n\n\n<img src=\"/img/blog19/0_1.webp\" alt=\"All possible unique values for \'ethnicity\' (Part 1)\" width=\"755\" height=\"1203\">\n<img src=\"/img/blog19/0_2.webp\" alt=\"All possible unique values for \'ethnicity\' (Part 2)\" width=\"754\" height=\"1339\">\n<img src=\"/img/blog19/0_3.webp\" alt=\"All possible unique values for \'ethnicity\' (Part 3)\" width=\"753\" height=\"1256\">\n\n\n\n\n<p>As you can see above, the ethnicity feature can take too many possible values, creating broader groups to reduce the number of possible categories.</p>\n\n\n\n\n\n\n<h2>Grouping Ethnicities into Strata for Machine Learning</h2>\n\n<p>In machine learning, each category is called a \'<strong>stratum</strong> \'. Each <a rel=\'noreferrer nofollow noopener\' href=\'https://medium.com/analytics-vidhya/stratified-sampling-in-machine-learning-f5112b5b9cfe\' target=\'_blank\'>stratum</a> should have sufficient instances. Otherwise, the estimate of a stratum\'s importance may be biased by too little non-representative data.<br/><br/>\n\nWe execute the code below to create a new column named classified_ethnicity and will follow the programming rules below to classify the entry into one specific ethnicity classification:<br/><br/>\n\n./chapter2/data2/data_manipulations2/step3.py</p>\n\n\n\n<pre><code class=\"language-python\">import os\nimport pandas as pd\n\n# Define the path to the dataset folder\nDATING_PATH = os.path.join(\"datasets\", \"dating\")\n\n# Function to load the dataset (male-only, straight)\ndef load_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"male_straight_data_with_persistent_id.csv\")\n    return pd.read_csv(csv_path)\n\n# Function to classify ethnicity\ndef classify_ethnicity(ethnicity):\n    # Handle non-string, NaN, or empty string values\n    if not isinstance(ethnicity, str) or pd.isna(ethnicity) or ethnicity.strip() == \'\':\n        return \'not declared\'\n    \n    # Split the ethnicity string by commas\n    ethnicities = [eth.strip().lower() for eth in ethnicity.split(\',\')]\n    \n    # Classify based on the number of ethnicities\n    if len(ethnicities) == 1:\n        return ethnicities[0]\n    elif len(ethnicities) == 2:\n        return ethnicities[0]\n    else:\n        return \'other\'\n\n# Load the dataset\nmale_data = load_dating_data()\n\n# Apply the ethnicity classification and add it as a new column\nmale_data[\'classified_ethnicity\'] = male_data[\'ethnicity\'].apply(classify_ethnicity)\n\n# Save the updated dataset with the new column\noutput_path = os.path.join(DATING_PATH, \"male_straight_data_with_persistant_id_classified_ethnicity.csv\")\nmale_data.to_csv(output_path, index=False)\n\nprint(f\"Dataset with classified ethnicity saved to: {output_path}\")</code></pre>\n\n\n\n\n\n\n\n<p>Here is a snip of the data before and after classification:</p>\n\n<img src=\"/img/blog19/1.webp\" alt=\"Ethnicity classification table before and after classification\" width=\"278\" height=\"739\">\n\n\n\n\n\n\n<h2>Visualizing Classified Ethnicities</h2>\n\n<p>After the classification is established, we can proceed to plot the graph. This gives us an idea of the proportions of groups of people represented in our <a rel=\'noreferrer nofollow noopener\' href=\'https://kili-technology.com/data-labeling/machine-learning/create-dataset-for-machine-learning\' target=\'_blank\'> dataset </a>.<br/><br/>\n\n\nWe use the code below to generate the plots.<br/><br/>\n\n\n./chapter2/data2/plots/plot1.py</p>\n\n\n\n\n<pre><code class=\"language-python\">import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset from the CSV file\nfile_path = \"datasets/dating/male_straight_data_with_persistant_id_classified_ethnicity.csv\"\ndata = pd.read_csv(file_path)\n\n# Verify the data loaded correctly\nprint(data.head())\n\n# Plot a histogram of the \'classified_ethnicity\' column\nplt.figure(figsize=(10, 6))  # Adjust the size of the plot\ndata[\'classified_ethnicity\'].value_counts().plot(kind=\'bar\', color=\'skyblue\', edgecolor=\'black\')\n\n# Add titles and labels\nplt.title(\'Distribution of Classified Ethnicities\', fontsize=16)\nplt.xlabel(\'Classified Ethnicity\', fontsize=12)\nplt.ylabel(\'Count\', fontsize=12)\n\n# Rotate x-axis labels for better readability\nplt.xticks(rotation=45, ha=\'right\')\n\n# Display the plot\nplt.show()</code></pre>\n\n\n\n\n\n<img src=\"/img/blog19/2.webp\" alt=\"Histogram: Distribution of Classified Ethnicities\" width=\"950\" height=\"629\">\n\n\n\n\n<h2>Performing a Stratified Split</h2>\n\n<p>We can now do a <a rel=\'noreferrer nofollow noopener\' href=\'https://www.geeksforgeeks.org/stratified-random-sampling-an-overview/\' target=\'_blank\'><strong>stratified split</strong></a> based on the classified ethnicity feature and maintain the same ratios in our <a rel=\'noreferrer nofollow noopener\' href=\'https://www.javatpoint.com/train-and-test-datasets-in-machine-learning\' target=\'_blank\'>test and training data</a>:<br/><br/>\n\n\n\nWe use <a rel=\'noreferrer nofollow noopener\' href=\'https://www.nvidia.com/en-us/glossary/scikit-learn/\' target=\'_blank\'>Scikit-Learn\'s</a> <a rel=\'noreferrer nofollow noopener\' href=\'https://www.rasgoml.com/feature-engineering-tutorials/how-to-do-scikit-learn-stratified-cross-validation-splits\' target=\'_blank\'>StratifiedShuffleSplit</a> class:<br/><br/>\n\n\n./chapter2/data2/data_manipulations2/step4.py</p>\n\n\n<pre><code class=\"language-python\">from sklearn.model_selection import StratifiedShuffleSplit\nimport pandas as pd\n\n# Load the dataset\nfile_path = \"datasets/dating/male_straight_data_with_persistant_id_classified_ethnicity.csv\"\ndating_data = pd.read_csv(file_path)\n\n# Verify the dating_data loaded correctly\nprint(dating_data.head())\n\n# Initialize StratifiedShuffleSplit with one split and 20% test size\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\n# Perform stratified split based on \'classified_ethnicity\'\nfor train_index, test_index in split.split(dating_data, dating_data[\'classified_ethnicity\']):\n    strat_train_set = dating_data.loc[train_index]\n    strat_test_set = dating_data.loc[test_index]\n\n\n# Save the persistent IDs to CSV files for future use\nstrat_train_set[\'persistent_id\'].to_csv(\'datasets/dating/male_straight_data_with_persistant_id_classified_ethnicity_stratified_train_ids.csv\', index=False)\nstrat_test_set[\'persistent_id\'].to_csv(\'datasets/dating/male_straight_data_with_persistant_id_classified_ethnicity_stratified_test_ids.csv\', index=False)\n\nprint(\"Train and test sets saved with persistent IDs.\")\n\n# Verify the size of the split datasets\nprint(f\"Training set size: {len(strat_train_set)}\")\nprint(f\"Test set size: {len(strat_test_set)}\")\n\n# Optional: Check the distribution of classified ethnicities in train and test sets\nprint(\"Training set ethnicity distribution:\")\nprint(strat_train_set[\'classified_ethnicity\'].value_counts(normalize=True))\n\nprint(\"\\nTest set ethnicity distribution:\")\nprint(strat_test_set[\'classified_ethnicity\'].value_counts(normalize=True))\n\n# Save the train and test sets with proper names\ntrain_file_path = \"datasets/dating/male_straight_data_with_persistant_id_classified_ethnicity_stratified_train_set.csv\"\ntest_file_path = \"datasets/dating/male_straight_data_with_persistant_id_classified_ethnicity_stratified_test_set.csv\"\n\nstrat_train_set.to_csv(train_file_path, index=False)\nstrat_test_set.to_csv(test_file_path, index=False)\n\n# Check the overall distribution of classified ethnicities in the original dataset\nprint(\"Overall ethnicity distribution (entire dataset):\")\nprint(dating_data[\'classified_ethnicity\'].value_counts(normalize=True))\n\nprint(f\"Training set saved to: {train_file_path}\")\nprint(f\"Test set saved to: {test_file_path}\")</code></pre>\n\n\n\n\n\n<p>The image below shows the distributions of the total, train, and <strong>test datasets</strong> based on the classified_ethnicity feature.</p>\n\n\n<img src=\"/img/blog19/3.webp\" alt=\"Ethnicity distribution in training, test, and entire datasets\" width=\"499\" height=\"841\">\n\n\n<p>Notice the consistency of proportions across all total datasets and <strong>split</strong> test and train datasets.</p>\n\n\n<h2>Rebooting the Dataset After Deletions</h2>\n\n<p>If after a re-split, after some entries got deleted, we want to reboot the same entries in each test/train group, we can leverage the script below to do so:<br/><br/>\n\n./chapter2/data2/data_manipulations2/step5.py</p>\n\n\n<pre><code class=\"language-python\">import pandas as pd\n\n# Load the updated dataset (after deletions or modifications)\nfile_path = \"datasets/dating/male_straight_data_with_persistant_id_classified_ethnicity.csv\"\nupdated_data = pd.read_csv(file_path)\n\n# Load the saved train and test IDs\ntrain_ids = pd.read_csv(\'train_ids.csv\')[\'persistent_id\']\ntest_ids = pd.read_csv(\'test_ids.csv\')[\'persistent_id\']\n\n# Restore the original train and test sets using the persistent IDs\nrestored_train_set = updated_data[updated_data[\'persistent_id\'].isin(train_ids)]\nrestored_test_set = updated_data[updated_data[\'persistent_id\'].isin(test_ids)]\n\n# Display the restored sets\nprint(\"Restored Training Set:\")\nprint(restored_train_set)\n\nprint(\"\\nRestored Test Set:\")\nprint(restored_test_set)</code></pre>\n\n\n\n\n<h2>Conclusion</h2>\n\n<p>We\'re ready to proceed with the classified ethnicity feature established and balanced through <strong>stratified sampling</strong>. In the next part of this series, we\'ll explore data visualization techniques to uncover patterns and insights hidden within the dataset.</p>\n\n<h2>Appendix</h2>\n\n<img src=\"/img/blog19/4.webp\" alt=\"CSV files containing stratified train and test sets\" width=\"810\" height=\"187\">\n\n<ul>\n<li>Data Labels</li>\n<li>Yellow: Original data used for the split</li>\n<li>Red: After split IDs</li>\n<li>Gray: Test or Train Data</li>\n</ul>\n\n<h2>Links to other parts of the project</h2>\n<ul>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/okcupid-dataset-analysis-for-machine-learning\">OkCupid Dataset Analysis for Machine Learning</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/exploring-okcupid-dataset-with-machine-learning-predicting-mating-success-for-straight-males\">Exploring OkCupid Dataset with Machine Learning: Predicting Mating Success for Straight Males</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/discover-and-visualize-the-data-to-gain-insights-on-feature-vs-label-correlations-okcupid-dataset\">Discover and Visualize the Data to Gain Insights on Feature vs Label Correlations: OkCupid Dataset</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/okcupid-dataset-preparing-the-data-before-learning-algorithms\">OkCupid Dataset: Preparing the Data Before Learning Algorithms</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/comparing-model-performance-linear-regression-decision-tree-and-random-forest-for-predicting-mating-success\">Comparing Model Performance: Linear Regression, Decision Tree, and Random Forest for Predicting Mating Success</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/comparing-model-performance-on-training-data-vs-test-data\">Comparing Model Performance on Training Data vs. Test Data</a></li>\n</ul>','2024-10-20 15:21:41','2024-11-23 18:00:40','exploring-okcupid-dataset-with-stratified-sampling',6,'Learn how to apply stratified sampling on the OkCupid dataset to maintain balanced feature representation. Explore the steps from grouping ethnicities into strata, visualizing distributions, and performing consistent train-test splits to ensure reliable machine learning predictions.','Ensuring Balanced Representation with Stratified Sampling for Improved Machine Learning Models','en',_binary '\0'),(20,'Discover and Visualize the Data to Gain Insights on Feature vs Label Correlations: OkCupid Dataset','<h2>Identifying Correlations</h2>\n\n<h3>Refining Offspring Categories for Clarity in Analysis</h3>\n\n<p>We first need to merge some classes of <a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/Feature_(machine_learning)\' target=\'_blank\'>features</a> into broader classes. That way, we can better capture the big picture in the plots we generate.<br/><br/>\n\n\n./chapter2/data2/load3.py</p>\n\n\n<pre><code class=\"language-python\">import os\nimport pandas as pd\n\nDATING_PATH = os.path.join(\"datasets\", \"dating\")\n\ndef load_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"male_straight_data_with_classified_columns.csv\")\n    return pd.read_csv(csv_path)\n\ndating_data = load_dating_data()\n\nprint(f\"\\nTotal entries in the dataset: {len(dating_data)}\")\n\n\ntarget_columns = [\'status\', \'offspring\']\n\nprint(\"\\nValue counts for specified columns:\")\nfor column in target_columns:\n    if column in dating_data.columns:\n        print(f\"\\n{column} value counts:\")\n        print(dating_data[column].value_counts(dropna=False))\n    else:\n        print(f\"\\nWarning: {column} not found in the dataset.\")\n\n\nprint(f\"\\nTotal entries in the dataset: {len(dating_data)}\")\n\nprint(\"\\nNumber of non-missing rows per column:\")\nfor column in dating_data.columns:\n    non_missing_count = dating_data[column].notna().sum()\n    print(f\"{column}: {non_missing_count} non-missing rows\")</code></pre>\n\n\n\n\n<h3>Classifying Offspring: Applying Consistent Rules</h3>\n\n<p>Below are the possible values for \"offspring\":</p>\n\n\n<pre><code class=\"language-python\">offspring value counts:\noffspring\nNaN                                        18927\ndoesn\'t have kids                           4192\ndoesn\'t have kids, but might want them      2128\ndoesn\'t have kids, but wants them           1622\ndoesn\'t want kids                           1228\nhas kids                                     912\nhas a kid                                    835\ndoesn\'t have kids, and doesn\'t want any      519\nhas kids, but doesn\'t want more              180\nhas a kid, and might want more               139\nmight want kids                               98\nhas a kid, but doesn\'t want more              93\nwants kids                                    77\nhas kids, and might want more                 67\nhas a kid, and wants more                     44\nhas kids, and wants more                      12\nName: count, dtype: int64</code></pre>\n\n\n\n<!-- <img src=\'/img/blog20/1.webp\' alt=\'\' target=\'_blank\' width=\'501\' height=\'auto\'/> --> \n\n\n\n\n<p>We need to classify offspring based on specific rules.<br/><br/>\n\nWe use this <a rel=\'noreferrer nofollow noopener\' href=\'https://www.altexsoft.com/blog/how-to-organize-data-labeling-for-machine-learning-approaches-and-tools/\' target=\'_blank\'>classification map</a> to classify all possible entry values for offspring:<br/><br/>\n\nThe logic is that 0 is assigned to people with no children, 1 is assigned to people with one kid, and 2 is assigned to people with at least two kids.</p>\n\n<pre><code class=\"language-python\">classification_map = {\n    \"doesn\'t have kids\": 0,\n    \"doesn\'t have kids, but might want them\": 0,\n    \"doesn\'t have kids, but wants them\": 0,\n    \"doesn\'t want kids\": 0,\n    \"doesn\'t have kids, and doesn\'t want any\": 0,\n    \"might want kids\": 0,\n    \"wants kids\": 0,\n    \"has a kid\": 1,\n    \"has a kid, and might want more\": 1,\n    \"has a kid, but doesn\'t want more\": 1,\n    \"has a kid, and wants more\": 1,\n    \"has kids\": 2,\n    \"has kids, but doesn\'t want more\": 2,\n    \"has kids, and might want more\": 2,\n    \"has kids, and wants more\": 2,\n}</code></pre>\n\n\n\n\n\n\n<p>We execute the code below to generate the new column that broadens the categories. This permits us to have a lower number of groups in total. The new groups are sufficiently distinctive from one another, so the loss in terms of specificity is not as lost:<br/><br/>\n\n\n\n./chapter2/data2/data_manipulations2/step6.py</p>\n\n\n\n<pre><code class=\"language-python\">import os\nimport pandas as pd\n\n# Define the path to the dataset folder\nDATING_PATH = os.path.join(\"datasets\", \"dating/copies2\")\n\n# Function to load the dataset (male-only, straight)\ndef load_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"copy1_male_straight_data_with_classified_columns.csv\")\n    return pd.read_csv(csv_path)\n\n# Function to classify offspring based on your provided rules\ndef classify_offspring(offspring):\n    if pd.isna(offspring):\n        return float(\'nan\')  # Keep NaN as is\n\n    # Mapping specific offspring values to corresponding classifications\n    classification_map = {\n        \"doesn\'t have kids\": 0,\n        \"doesn\'t have kids, but might want them\": 0,\n        \"doesn\'t have kids, but wants them\": 0,\n        \"doesn\'t want kids\": 0,\n        \"doesn\'t have kids, and doesn\'t want any\": 0,\n        \"might want kids\": 0,\n        \"wants kids\": 0,\n        \"has a kid\": 1,\n        \"has a kid, and might want more\": 1,\n        \"has a kid, but doesn\'t want more\": 1,\n        \"has a kid, and wants more\": 1,\n        \"has kids\": 2,\n        \"has kids, but doesn\'t want more\": 2,\n        \"has kids, and might want more\": 2,\n        \"has kids, and wants more\": 2,\n    }\n\n    # Standardizing the input string to match the keys in the map\n    offspring = offspring.strip().lower()\n    \n    # Return the corresponding classification or NaN if not found\n    return classification_map.get(offspring, float(\'nan\'))\n\n# Load the dataset\nmale_data = load_dating_data()\n\n# Apply the offspring classification and add it as a new column\nmale_data[\'classified_offspring\'] = male_data[\'offspring\'].apply(classify_offspring)\n\n# Save the updated dataset with the new column\noutput_path = os.path.join(DATING_PATH, \"copy2_male_straight_data_with_classified_columns_classify_offspring.csv\")\n\nmale_data.to_csv(output_path, index=False)\n\nprint(f\"Dataset with classified offspring saved to: {output_path}\")</code></pre>\n\n\n\n\n<h3>Classifying Relationship Status for Better Insights</h3>\n\n\n\n\n<p>We now need to classify \"status\", and these are the possible values for \"status\":</p>\n\n\n<pre><code class=\"language-python\">status value counts:\nstatus\nsingle            29163\navailable           899\nseeing someone      854\nmarried             152\nunknown               5\nName: count, dtype: int64</code></pre>\n\n\n\n<!-- <img src=\'/img/blog20/2.webp\' alt=\'\' target=\'_blank\' width=\'266\' height=\'auto\'/> -->\n\n\n\n\n\n<p>We use this classification map to classify all possible entry values for status based on specific rules.<br/><br/>\n\nThe logic is that 0 is assigned to singles. 1 is assigned to people with a status of \'available.\' The status \'available\' implies not being single and being open to mating options. <br/><br/>\n\n\'Seeing someone\' and \'married\' both mean that someone is consistently engaging with a partner; therefore, a <a rel=\'noreferrer nofollow noopener\' href=\'https://aws.amazon.com/what-is/data-labeling/\' target=\'_blank\'>label</a> of 2 will be assigned.</p>\n\n\n\n<pre><code class=\"language-python\">classification_map = {\n    \"single\": 0,\n    \"available\": 1,\n    \"seeing someone\": 2,\n    \"married\": 2,\n    \"unknown\": float(\'nan\')  # Handle unknown cases as NaN\n}</code></pre>\n\n\n\n\n<h2>Transforming Relationship Status into Categorical Values</h2>\n\n\n\n<p>We execute the code below to generate the new column of numbered statuses:<br/><br/>\n\n\n./chapter2/data2/data_manipulations2/step7.py</p>\n\n\n<pre><code class=\"language-python\">import os\nimport pandas as pd\n\n# Define the path to the dataset folder\nDATING_PATH = os.path.join(\"datasets\", \"dating/copies2\")\n\n# Function to load the dataset (male-only, straight)\ndef load_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"copy2_male_straight_data_with_classified_columns_classify_offspring.csv\")\n    return pd.read_csv(csv_path)\n\n# Function to classify status based on the provided mapping\ndef classify_status(status):\n    if pd.isna(status):\n        return float(\'nan\')  # Keep NaN as is\n\n    # Mapping specific status values to corresponding classifications\n    classification_map = {\n        \"single\": 0,\n        \"available\": 1,\n        \"seeing someone\": 2,\n        \"married\": 2,\n        \"unknown\": float(\'nan\')  # Handle unknown cases as NaN\n    }\n\n    # Standardizing the input string to match the keys in the map\n    status = status.strip().lower()\n    \n    # Return the corresponding classification or NaN if not found\n    return classification_map.get(status, float(\'nan\'))\n\n# Load the dataset\nmale_data = load_dating_data()\n\n# Apply the status classification and add it as a new column\nmale_data[\'classified_status\'] = male_data[\'status\'].apply(classify_status)\n\n# Save the updated dataset with the new column\noutput_path = os.path.join(DATING_PATH,\"copy3_male_straight_data_with_classified_columns_classify_offspring_classified_status.csv\")\nmale_data.to_csv(output_path, index=False)\n\nprint(f\"Dataset with classified status saved to: {output_path}\")</code></pre>\n\n\n\n\n\n<h2>Investigating Key Feature Correlations with Mating Success</h2>\n\n<p>We study the correlations of the label (mating_success) vs other features (age, classified_ethnicity, height, income, classified_body_type, classified_drinks, classified_smokes, classified_drugs, classified_religion, classified_speaks) to understand the data we have.<br/><br/>\n\nThe label is called \'mating_success\' and scales from 0 to 6.<br/><br/>\n\nThe <strong>label</strong> is computed based on the value of the entries\' offspring\' and \'status\' <strong>features</strong>. These features are crucial in understanding the individual\'s family and relationship status, which can significantly influence their mating success. <br/><br/>\n\nBelow is the code used to compute the label\'s (mating_success) value:<br/><br/>\n\n\n./chapter2/data2/data_manipulations2/step8_1.py</p>\n\n<pre><code class=\"language-python\">import os\nimport pandas as pd\n\n# Define the path to the dataset folder\nDATING_PATH = os.path.join(\"datasets\", \"dating/copies2\")\n\n# Function to load the dataset (male-only, straight)\ndef load_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"copy3_male_straight_data_with_classified_columns_classify_offspring_classified_status.csv\")\n    return pd.read_csv(csv_path)\n\n# Function to compute mating_success based on the specified logic\ndef compute_mating_success(row):\n    status = row[\'classified_status\']\n    offspring = row[\'classified_offspring\']\n\n    # Handle cases based on the new logic\n    if pd.isna(status) and pd.isna(offspring):\n        return float(\'nan\')\n    elif pd.isna(status) and not pd.isna(offspring):\n        return offspring * 2\n    elif not pd.isna(status) and pd.isna(offspring):\n        if status == 0:\n            return float(\'nan\')  # status is 0 -> mating_success is NaN\n        elif status == 1:\n            return 1  # status is 1 -> mating_success is 1\n        elif status == 2:\n            return 2  # status is 2 -> mating_success is 2\n    else:\n        # If both status and offspring are present\n        return status + offspring * 2\n\n# Load the dataset\nmale_data = load_dating_data()\n\n# Apply the function to calculate mating_success\nmale_data[\'mating_success\'] = male_data.apply(compute_mating_success, axis=1)\n\n# Save the updated dataset with the new column\noutput_path = os.path.join(DATING_PATH, \"copy4_classified_features_classified_label.csv\")\nmale_data.to_csv(output_path, index=False)\n\nprint(f\"Dataset with mating_success saved to: {output_path}\")</code></pre>\n\n\n\n\n<p>The first features to check <strong>correlations</strong> with the label will be height, income, and age.<br/><br/>\n\n./chapter2/data2/correlations/corr1.py</p>\n\n<pre><code class=\"language-python\">import os\nimport pandas as pd\n\n# Define the path where the dataset is located\nDATING_PATH = \"/Users/Zouhir/Documents/UQO_AUT2024/INF6333_code/book_aurelien_geron/chapter2/datasets/dating/copies\"\n\ndef load_dating_data(dating_path=DATING_PATH):\n    \"\"\"\n    Load the dating data from a CSV file.\n    \"\"\"\n    csv_path = os.path.join(dating_path, \"copy4_mating_success_1.csv\")\n    return pd.read_csv(csv_path)\n\n# Load the dating data\ndating = load_dating_data()\n\n# Select only numeric columns from the dating DataFrame\nnumeric_dating = dating.select_dtypes(include=[float, int])\n\n# Calculate the correlation matrix\ncorr_matrix = numeric_dating.corr()\n\n# Display the correlations with \'median_house_value\', sorted in descending order\nprint(corr_matrix[\"mating_success\"].sort_values(ascending=False))</code></pre>\n\n\n<pre><code class=\"language-python\">mating_success          1.000000\nclassified_offspring    0.966118\nage                     0.477575\nclassified_status       0.378283\nincome                  0.021577\nheight                  0.008759\nName: mating_success, dtype: float64</code></pre>\n\n\n\n\n<!-- <img src=\'/img/blog20/3.webp\' alt=\'\' target=\'_blank\' width=\'388\' height=\'auto\'/> -->\n\n\n\n<p>Age is the most correlated factor with mating success, out of age, income, and height.</p>\n\n\n\n\n<h2>Comprehensive Feature Analysis for Mating Success Patterns</h2>\n\n\n\n<p>This code analyzes how various <strong>features correlate</strong> with the mating_success <strong>label</strong>. It loads a dataset containing information on individuals, such as their age, ethnicity, height, income, and lifestyle factors, and splits the features into numeric and categorical types for separate analysis.<br/><br/>\n\nFor numeric features (like age, height, and income), the <strong>code</strong> generates <strong><a rel=\'noreferrer nofollow noopener\' href=\'https://www.atlassian.com/data/charts/what-is-a-scatter-plot\' target=\'_blank\'>scatter plots</a></strong> against the mating_success label, helping to visualize potential correlations. For categorical features, it calculates <strong><a rel=\'noreferrer nofollow noopener\' href=\'https://www.codecademy.com/article/normalization\' target=\'_blank\'>normalized proportions</a></strong> to account for class imbalances and displays the distribution of mating_success across each category using <strong><a rel=\'noreferrer nofollow noopener\' href=\'https://www.geeksforgeeks.org/create-a-stacked-bar-plot-in-matplotlib/\' target=\'_blank\'>stacked bar plots</a></strong>. Additionally, it plots the <strong>mean</strong> mating success for each category to highlight differences in average outcomes across feature classes.<br/><br/>\n\nThis analysis provides a structured visual approach to explore how different features may impact or correlate with mating_success, offering insights into <strong>patterns</strong> within the <strong><a rel=\'noreferrer nofollow noopener\' href=\'https://labelyourdata.com/articles/what-is-dataset-in-machine-learning\' target=\'_blank\'>dataset</a></strong>.<br/><br/>\n\n\n./chapter2/data2/data_visualize/load4.py</p>\n\n<pre><code class=\"language-python\"># Analyse data of people with all types of mating success or not with proportions per class taken into account\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset\nDATING_PATH = os.path.join(\"datasets\", \"dating/copies3\")\ndef load_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"5_classified_features_needed_label.csv\")\n    return pd.read_csv(csv_path)\n\ndating_data = load_dating_data()\n\n# Print the columns to verify\nprint(dating_data.columns)\n\n# Define the label and features\nlabel = \'mating_success\'\nfeatures = [\'age\', \'classified_ethnicity\', \'height\', \'income\',\n            \'classified_body_type\', \'classified_drinks\', \'classified_smokes\', \'classified_drugs\',\n            \'classified_religion\', \'classified_speaks\']\n\nif label in dating_data.columns:\n    # Separate numeric and categorical features for different plotting methods\n    numeric_features = [\'age\', \'height\', \'income\']\n    categorical_features = [feature for feature in features if feature not in numeric_features]\n\n    # Plot numeric features vs. label with scatter plots\n    plt.figure(figsize=(15, 5 * len(numeric_features)))\n    for i, feature in enumerate(numeric_features, 1):\n        plt.subplot(len(numeric_features), 1, i)\n        sns.scatterplot(data=dating_data, x=feature, y=label, alpha=0.5)\n        plt.xlabel(feature)\n        plt.ylabel(label)\n        plt.title(f\'Scatter Plot of {feature} vs. {label}\')\n    plt.tight_layout()\n    plt.show()\n\n    # Plot categorical features with normalized counts (proportions)\n    for feature in categorical_features:\n        # Calculate proportions\n        prop_df = dating_data.groupby([feature, label]).size().unstack(fill_value=0)\n        prop_df = prop_df.div(prop_df.sum(axis=1), axis=0)  # Normalize by row\n\n        plt.figure(figsize=(12, 6))\n        prop_df.plot(kind=\"bar\", stacked=True, ax=plt.gca(), colormap=\"viridis\")\n        plt.title(f\'Proportion Plot of {feature} by {label}\')\n        plt.xlabel(feature)\n        plt.ylabel(\"Proportion\")\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.show()\n\n    # Plot mean `mating_success` for each category\n    for feature in categorical_features:\n        plt.figure(figsize=(12, 6))\n        mean_df = dating_data.groupby(feature)[label].mean().sort_values()\n        sns.barplot(x=mean_df.index, y=mean_df.values)\n        plt.title(f\'Mean {label} by {feature}\')\n        plt.xlabel(feature)\n        plt.ylabel(f\'Mean {label}\')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.show()\nelse:\n    print(f\"Warning: \'{label}\' column is missing from the dataset.\")</code></pre>\n\n\n\n\n<h2>OkCupid Dataset: Visualizing correlations between age, ethnicity, religion, height, income, body type, and substances use with mating success</h2>\n\n\n<p>Note that the feature \'mating sucess\' captures more of the person\'s ability to consolidate a long term relationship, and child, as opposed to \'casual hookups\'.<br/><br/>\n\n\nNote that sampling bias is because the data is from a dating app, affecting the low mating success in males on the app. The people on the app are supposed to be single as they purposefully use it to either find someone or, on fewer occasions, \'cheat.\'</p>\n\n<img src=\'/img/blog20/4.webp\' alt=\'Scatter plot showing age, income, and height versus mating success\' target=\'_blank\' width=\'700\' height=\'auto\'/>\n\n<img src=\'/img/blog20/5.webp\' alt=\'Proportion plot of classified ethnicity by mating success\' target=\'_blank\' width=\'700\' height=\'auto\'/>\n\n<img src=\'/img/blog20/6.webp\' alt=\'Proportion plot of classified body type by mating success\' target=\'_blank\' width=\'700\' height=\'auto\'/>\n\n<img src=\'/img/blog20/7.webp\' alt=\'Proportion plot of classified drinks by mating success\' target=\'_blank\' width=\'700\' height=\'auto\'/>\n\n<img src=\'/img/blog20/8.webp\' alt=\'Proportion plot of classified smokes by mating success\' target=\'_blank\' width=\'700\' height=\'auto\'/>\n\n<img src=\'/img/blog20/9.webp\' alt=\'Proportion plot of classified drugs by mating success\' target=\'_blank\' width=\'700\' height=\'auto\'/>\n\n<img src=\'/img/blog20/10.webp\' alt=\'Proportion plot of classified religion by mating success\' target=\'_blank\' width=\'700\' height=\'auto\'/>\n\n<img src=\'/img/blog20/11.webp\' alt=\'Proportion plot of classified speaks by mating success\' target=\'_blank\' width=\'700\' height=\'auto\'/>\n\n<img src=\'/img/blog20/12.webp\' alt=\'Bar chart showing average mating success across various ethnic classifications.\' target=\'_blank\' width=\'700\' height=\'auto\'/>\n\n<img src=\'/img/blog20/13.webp\' alt=\'Bar chart depicting average mating success by different body types.\' target=\'_blank\' width=\'700\' height=\'auto\'/>\n\n<img src=\'/img/blog20/14.webp\' alt=\'Bar chart comparing average mating success between drinkers and non-drinkers.\' target=\'_blank\' width=\'700\' height=\'auto\'/>\n\n<img src=\'/img/blog20/15.webp\' alt=\'Bar chart displaying average mating success between smokers and non-smokers.\' target=\'_blank\' width=\'700\' height=\'auto\'/>\n\n<img src=\'/img/blog20/16.webp\' alt=\'Bar chart indicating average mating success for drug users versus non-users.\' target=\'_blank\' width=\'700\' height=\'auto\'/>\n\n<img src=\'/img/blog20/17.webp\' alt=\'Bar chart showing average mating success by different religious classifications.\' target=\'_blank\' width=\'700\' height=\'auto\'/>\n\n<img src=\'/img/blog20/18.webp\' alt=\'Bar chart comparing average mating success between English speakers and those speaking other languages.\' target=\'_blank\' width=\'700\' height=\'auto\'/>\n\n\n<h2>Links to other parts of the project</h2>\n\n<ul>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/okcupid-dataset-analysis-for-machine-learning\">OkCupid Dataset Analysis for Machine Learning</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/exploring-okcupid-dataset-with-machine-learning-predicting-mating-success-for-straight-males\">Exploring OkCupid Dataset with Machine Learning: Predicting Mating Success for Straight Males</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/exploring-okcupid-dataset-with-stratified-sampling\">Exploring OkCupid Dataset with Stratified Sampling</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/okcupid-dataset-preparing-the-data-before-learning-algorithms\">OkCupid Dataset: Preparing the Data Before Learning Algorithms</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/comparing-model-performance-linear-regression-decision-tree-and-random-forest-for-predicting-mating-success\">Comparing Model Performance: Linear Regression, Decision Tree, and Random Forest for Predicting Mating Success</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/comparing-model-performance-on-training-data-vs-test-data\">Comparing Model Performance on Training Data vs. Test Data</a></li>\n</ul>','2024-11-03 17:31:59','2024-11-30 18:00:40','discover-and-visualize-the-data-to-gain-insights-on-feature-vs-label-correlations-okcupid-dataset',6,'Explore the correlation between various features, such as age, height, income, ethnicity, and lifestyle choices, with mating success using data from the OkCupid dataset. This analysis includes visualizations like scatter plots and bar charts, highlighting patterns in relationship and family dynamics, as well as the impact of social behaviors like drinking and smoking. Dive into how these factors influence long-term relationship success and progeny outcomes in the context of dating trends.','OkCupid Dataset','en',_binary '\0'),(21,'OkCupid Dataset: Preparing the Data Before Learning Algorithms','<h2>Effective Data Preprocessing with Python: Handling Missing Values, Scaling, and Data Inspection</h2>\n\n<p><a rel=\'noreferrer nofollow noopener\' href=\'https://www.geeksforgeeks.org/data-preprocessing-in-data-mining/\' target=\'_blank\'>Data preprocessing</a> is crucial in any data science pipeline, especially when working with real-world <a rel=\'noreferrer nofollow noopener\' href=\'https://en.wikipedia.org/wiki/Data_set\' target=\'_blank\'>datasets</a> that often contain missing values, unscaled features, and categorical variables.<br/><br/>\n\nIn this blog, we\'ll describe several steps to clean, scale, and inspect a dating dataset using Python. The dataset includes numerical and categorical features and a target variable to predict.<br/><br/>\n\nWe\'ll demonstrate how to handle missing data, perform scaling on numerical columns, and inspect the original and preprocessed data for discrepancies.</p>\n\n\n\n\n<h2>Loading and Splitting the Data</h2>\n\n<p>We start by loading a dating dataset and separating it into <a rel=\'noreferrer nofollow noopener\' href=\'https://stackoverflow.com/questions/40898019/what-is-the-difference-between-a-feature-and-a-label\' target=\'_blank\'>features and labels</a>.<br/><br/>\n\nThe goal here is to load the data, isolate the features (input variables), and set the target label for prediction.<br/><br/>\n\n\n./chapter2/data2/data_cleaning/c1.py</p>\n\n\n<pre><code class=\"language-python\">import os\nimport pandas as pd\n\n# Define the path to the dataset\nDATING_PATH = \"./chapter2/datasets/dating/copies4\"\n\ndef load_dating_data(dating_path=DATING_PATH):\n    \"\"\"\n    Load the dating data from a CSV file.\n    \"\"\"\n    csv_path = os.path.join(dating_path, \"5_classified_features_needed_label_stratified_train_set.csv\")\n    return pd.read_csv(csv_path)\n\n# Load the training dataset\ndating_training_strat = load_dating_data()\n\n# Separate the features (input data) from the label (target variable)\ndating_training_features = dating_training_strat.drop(\"mating_success\", axis=1)  # Drop the label column to get the features\ndating_training_label = dating_training_strat[\"mating_success\"].copy()  # Copy the label column\n\n# Ensure persistent_id is part of both features and labels\npersistent_id = dating_training_strat[\"persistent_id\"]\n\n# Add persistent_id to both features and label\ndating_training_features[\'persistent_id\'] = persistent_id\ndating_training_label = pd.concat([persistent_id, dating_training_label], axis=1)  # Add persistent_id to labels\n\n# Print the first few rows of the features and labels to verify\nprint(\"Features (first 5 rows):\\n\", dating_training_features.head())\nprint(\"\\nLabels (first 5 rows):\\n\", dating_training_label.head())\n\n# Define file paths to save the features and label\nfeatures_path = os.path.join(DATING_PATH, \"5_classified_dating_features_train_set.csv\")\nlabel_path = os.path.join(DATING_PATH, \"5_classified_dating_label_train_set.csv\")\n\n# Save the features and label as separate CSV files\ndating_training_features.to_csv(features_path, index=False)\ndating_training_label.to_csv(label_path, index=False)\n\nprint(f\"Features saved to: {features_path}\")\nprint(f\"Labels saved to: {label_path}\")</code></pre>\n\n\n<p>In this step, we load the dataset, split it into features and the label (mating_success), and add persistent_id to both.<br/><br/>\n\nThe features and labels are then saved into separate CSV files for future use.</p>\n\n\n\n\n\n\n<h2>Handling Missing Data and Scaling</h2>\n\n<p>Once the data is loaded, we must clean and scale the numerical features.<br/><br/>\n\nThis involves several tasks, such as imputing missing values and standardizing features.<br/><br/>\n\n\n<a rel=\'noreferrer nofollow noopener\' href=\'https://scikit-learn.org/1.5/modules/impute.html\' target=\'_blank\'>Imputing</a> missing values is necessary to fill in the missing data with a reasonable estimate and avoid losing valuable information on other features.<br/><br/>\n\nWe use <a rel=\'noreferrer nofollow noopener\' href=\'https://www.analyticsvidhya.com/blog/2022/10/understand-the-concept-of-standardization-in-machine-learning/\' target=\'_blank\'>standardization</a> to scale numerical features to have a mean of 0 and a standard deviation of 1, ensuring features contribute equally to models and so that no transformation will not bound values to a specific range, unlike <a rel=\'noreferrer nofollow noopener\' href=\'https://databasecamp.de/en/ml/minmax-scaler-en\' target=\'_blank\'>min-max scaling</a>.<br/><br/>\n\nWe\'ll manually apply one-hot encoding for categorical variables to convert them into numerical representations.<br/><br/>\n\n<a rel=\'noreferrer nofollow noopener\' href=\'https://www.analyticsvidhya.com/blog/2023/12/how-to-do-one-hot-encoding/\' target=\'_blank\'>One-hot encoding</a> converts categorical variables into binary columns to make them suitable for machine-learning algorithms.<br/><br/>\n\n\n\n./chapter2/data2/data_cleaning3/simple3.py</p>\n\n\n<pre><code class=\"language-python\">import os\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Define the path to the dataset\nDATING_PATH = \"./chapter2/datasets/dating/copies4\"\n\ndef load_dating_data(dating_path=DATING_PATH):\n    \"\"\"\n    Load the dating data from a CSV file.\n    \"\"\"\n    csv_path = os.path.join(dating_path, \"5_1_classified_dating_features_train_set.csv\")\n    return pd.read_csv(csv_path)\n\n# Load the training dataset\ndating_data = load_dating_data()\n\n# -------------------\n# Cleaning Numeric Fields\n# -------------------\n\n# Initialize StandardScaler\nscaler = StandardScaler()\n\n# Define statistics to store for all numerical columns\nnumeric_columns_stats = []\n\n# Persistent ID: Do nothing\n# Age: Do nothing\n# Height: Impute missing values with mean\ndating_data[\'height\'].fillna(dating_data[\'height\'].mean(), inplace=True)\n# dating_data[\'age\'].fillna(dating_data[\'age\'].mean(), inplace=True)\n\n# Income: Replace -1 with mean\nincome_mean = dating_data[\'income\'][dating_data[\'income\'] != -1].mean()\ndating_data[\'income\'] = dating_data[\'income\'].replace(-1, income_mean)\n\n# List of numerical columns to scale\nnumeric_columns = [\'height\', \'income\', \'age\']\n\nfor col in numeric_columns:\n    # For each numerical column, compute the statistics and standardize the column\n    col_mean = dating_data[col].mean()\n    col_std = dating_data[col].std()\n\n    # Standardize the column (Z-score standardization)\n    dating_data[f\'scaled_{col}\'] = scaler.fit_transform(dating_data[[col]])\n\n    # Save the statistics\n    numeric_columns_stats.append({\n        \'Column\': col,\n        \'Mean (Imputation)\': dating_data[col].mean(),  # Mean used for imputation\n        \'Mean (Scaling)\': col_mean,                    # Mean used for standardization\n        \'Standard Deviation (Scaling)\': col_std        # Standard deviation used for standardization\n    })\n\n# -------------------------\n# Cleaning Categorical Fields\n# -------------------------\n\n# 1. Drop \'orientation\' and \'sex\'\ndating_data.drop([\'orientation\', \'sex\'], axis=1, inplace=True)\n\n# 2. Clean \'classified_ethnicity\': Drop \'not declared\' rows\ndating_data = dating_data[dating_data[\'classified_ethnicity\'] != \'not declared\']\n\n# Manually one-hot encode \'classified_ethnicity\'\nethnicity_categories = dating_data[\'classified_ethnicity\'].unique()\nfor category in ethnicity_categories:\n    dating_data[f\'ethnicity_{category}\'] = (dating_data[\'classified_ethnicity\'] == category).astype(int)\n\n# Drop original \'classified_ethnicity\' column\ndating_data.drop([\'classified_ethnicity\'], axis=1, inplace=True)\n\n# 3. Clean \'classified_body_type\': Fill missing values with \'average\'\ndating_data[\'classified_body_type\'].fillna(\'average\', inplace=True)\n\n# Manually one-hot encode \'classified_body_type\'\nbody_type_categories = dating_data[\'classified_body_type\'].unique()\nfor category in body_type_categories:\n    dating_data[f\'body_type_{category}\'] = (dating_data[\'classified_body_type\'] == category).astype(int)\n\n# Drop original \'classified_body_type\' column\ndating_data.drop([\'classified_body_type\'], axis=1, inplace=True)\n\n# 4. Clean \'classified_drinks\': Fill missing values with \'yes\'\ndating_data[\'classified_drinks\'].fillna(\'yes\', inplace=True)\n\n# Manually one-hot encode \'classified_drinks\'\ndrinks_categories = dating_data[\'classified_drinks\'].unique()\nfor category in drinks_categories:\n    dating_data[f\'drinks_{category}\'] = (dating_data[\'classified_drinks\'] == category).astype(int)\n\n# Drop original \'classified_drinks\' column\ndating_data.drop([\'classified_drinks\'], axis=1, inplace=True)\n\n# 5. Clean \'classified_smokes\': Fill missing values with \'No\'\ndating_data[\'classified_smokes\'].fillna(\'No\', inplace=True)\n\n# Manually one-hot encode \'classified_smokes\'\nsmokes_categories = dating_data[\'classified_smokes\'].unique()\nfor category in smokes_categories:\n    dating_data[f\'smokes_{category}\'] = (dating_data[\'classified_smokes\'] == category).astype(int)\n\n# Drop original \'classified_smokes\' column\ndating_data.drop([\'classified_smokes\'], axis=1, inplace=True)\n\n# 6. Clean \'classified_drugs\': Fill missing values with \'No\'\ndating_data[\'classified_drugs\'].fillna(\'No\', inplace=True)\n\n# Manually one-hot encode \'classified_drugs\'\ndrugs_categories = dating_data[\'classified_drugs\'].unique()\nfor category in drugs_categories:\n    dating_data[f\'drugs_{category}\'] = (dating_data[\'classified_drugs\'] == category).astype(int)\n\n# Drop original \'classified_drugs\' column\ndating_data.drop([\'classified_drugs\'], axis=1, inplace=True)\n\n# 7. Clean \'classified_religion\': Fill missing values with \'not declared\'\ndating_data[\'classified_religion\'].fillna(\'not declared\', inplace=True)\n\n# Manually one-hot encode \'classified_religion\'\nreligion_categories = dating_data[\'classified_religion\'].unique()\nfor category in religion_categories:\n    dating_data[f\'religion_{category}\'] = (dating_data[\'classified_religion\'] == category).astype(int)\n\n# Drop original \'classified_religion\' column\ndating_data.drop([\'classified_religion\'], axis=1, inplace=True)\n\n# 8. Clean \'classified_speaks\': Fill missing values with \'english\'\ndating_data[\'classified_speaks\'].fillna(\'english\', inplace=True)\n\n# Manually one-hot encode \'classified_speaks\'\nspeaks_categories = dating_data[\'classified_speaks\'].unique()\nfor category in speaks_categories:\n    dating_data[f\'speaks_{category}\'] = (dating_data[\'classified_speaks\'] == category).astype(int)\n\n# Drop original \'classified_speaks\' column\ndating_data.drop([\'classified_speaks\'], axis=1, inplace=True)\n\n# -----------------------------------\n# Save the cleaned dataset as a CSV\n# -----------------------------------\noutput_path = os.path.join(DATING_PATH, \"5_1_1_cleaned_dating_data.csv\")\ndating_data.to_csv(output_path, index=False)\n\nprint(f\"Cleaned dataset saved at: {output_path}\")\n\n# -------------------------------\n# Save the statistical values CSVs\n# -------------------------------\n\n# Convert the statistics list to a DataFrame\nstats_df = pd.DataFrame(numeric_columns_stats)\n\n# Save all statistics in a single CSV file\nstats_output_path = os.path.join(DATING_PATH, \"5_1_2_numerical_statistics.csv\")\nstats_df.to_csv(stats_output_path, index=False)\n\nprint(f\"All statistics saved at: {stats_output_path}\")</code></pre>\n\n\n\n\n<p>The cleaned dataset is then saved for later use.</p>\n\n\n\n\n<h2>Conclusion</h2>\n\n<p>Data preprocessing involves multiple steps, including handling missing data, scaling numerical features, encoding categorical variables, and verifying transformations.<br/><br/>\n\nFollowing these steps ensures that our dataset is clean, consistent, and ready for <a rel=\'noreferrer nofollow noopener\' href=\'/\' target=\'_blank\'>modelling</a>.<br/><br/>\n\nThe methods demonstrated in this blog offer a systematic approach to transforming raw data into a well-prepared dataset for <a rel=\'noreferrer nofollow noopener\' href=\'https://www.ibm.com/topics/machine-learning\' target=\'_blank\'>machine learning</a> tasks.</p>\n\n\n\n<h2>Links to other parts of the project</h2>\n\n<ul>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/okcupid-dataset-analysis-for-machine-learning\">OkCupid Dataset Analysis for Machine Learning</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/exploring-okcupid-dataset-with-machine-learning-predicting-mating-success-for-straight-males\">Exploring OkCupid Dataset with Machine Learning: Predicting Mating Success for Straight Males</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/exploring-okcupid-dataset-with-stratified-sampling\">Exploring OkCupid Dataset with Stratified Sampling</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/discover-and-visualize-the-data-to-gain-insights-on-feature-vs-label-correlations-okcupid-dataset\">Discover and Visualize the Data to Gain Insights on Feature vs Label Correlations: OkCupid Dataset</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/comparing-model-performance-linear-regression-decision-tree-and-random-forest-for-predicting-mating-success\">Comparing Model Performance: Linear Regression, Decision Tree, and Random Forest for Predicting Mating Success</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/comparing-model-performance-on-training-data-vs-test-data\">Comparing Model Performance on Training Data vs. Test Data</a></li>\n</ul>\n\n\n\n<h2>Annexe</h2>\n\n<h3>Inspecting Data for Quality Assurance</h3>\n\n<p>After preprocessing, it\'s important to ensure the transformations are applied correctly.<br/><br/>\n\nOne way to do this is by inspecting individual rows, especially using unique identifiers like persistent_id to track and compare original versus preprocessed data.<br/><br/>\n\nHere, we allow the user to input a persistent_id and then compare the corresponding row in the original and preprocessed datasets.<br/><br/>\n\nThis is a quality check to ensure that all transformations were applied correctly.<br/><br/>\n\n./chapter2/data2/comparator8.py</p>\n\n\n<pre><code class=\"language-python\">import os\nimport pandas as pd\n\n# Define the path where the dataset is located\nDATING_PATH = os.path.join(\"datasets\", \"dating/copies4/\")\n\ndef load_dating_data(dating_path=DATING_PATH, file_name=None):\n    \"\"\"Load the data from a CSV file.\"\"\"\n    csv_path = os.path.join(dating_path, file_name)\n    return pd.read_csv(csv_path)\n\n# Load the original and preprocessed data\ndating_data_original = load_dating_data(file_name=\"5_1_classified_dating_features_train_set.csv\")\n# dating_data_preprocessed = load_dating_data(file_name=\"5_1_3_preprocessed_dating_features.csv\")\ndating_data_preprocessed = load_dating_data(file_name=\"cleaned_dating_data.csv\")\n\n# Prompt user for a persistent_id\npersistent_id = input(\"Enter a persistent_id to search: \")\n\n# Check if the persistent_id exists in the preprocessed dataset\nif persistent_id in dating_data_preprocessed[\'persistent_id\'].values:\n    # Find the row in both datasets for the given persistent_id\n    original_row = dating_data_original[dating_data_original[\'persistent_id\'] == persistent_id]\n    preprocessed_row = dating_data_preprocessed[dating_data_preprocessed[\'persistent_id\'] == persistent_id]\n    \n    # Print the results for the user in a readable format\n    print(f\"\\nOriginal Data for persistent_id: {persistent_id}\")\n    print(\"\\nFields and values in original data:\")\n    for col, val in original_row.iloc[0].items():  # Iterate through the columns and values\n        print(f\"{col}: {val}\")\n\n    print(\"\\nPreprocessed Data for persistent_id: {persistent_id}\")\n    print(\"\\nFields and values in preprocessed data:\")\n    for col, val in preprocessed_row.iloc[0].items():  # Iterate through the columns and values\n        print(f\"{col}: {val}\")\n\nelse:\n    # Notify the user that the persistent_id was not found\n    print(f\"\\nError: persistent_id {persistent_id} does not exist in the preprocessed dataset.\")</code></pre>\n\n\n\n<h3>Files produced by code snippets</h3>\n\n\n<img src=\'/img/blog21/2.webp\' alt=\'Image of file system showing CVS dataset files produced by code.\' target=\'_blank\' width=\'600\' height=\'auto\'/>\n\n<h3>Labels:</h3>\n<ul>\n<li>Yellow: Training data (can delete orientation and sexe features)</li>\n<li>Green: Features (can delete orientation and sexe feature columns)</li>\n<li>Red: Labels</li>\n<li>Gray: Cleaned Data spawned from original file with green label (5_1_classified_dating_features_train_set.csv)</li>\n</ul>\n\n<p>Note: All important parameters used to compute the cleaned dataset are saved in 5_1_2_numerical_statistics.cvs</p>\n\n<img src=\'/img/blog21/3.webp\' alt=\'Numerical statistics used to compute the cleaned data\' target=\'_blank\' width=\'600\' height=\'auto\'/>','2024-11-19 16:18:19','2024-11-23 18:00:40','okcupid-dataset-preparing-the-data-before-learning-algorithms',6,'Learn effective data preprocessing techniques in Python using the OkCupid dataset. This guide covers handling missing values, scaling numerical features, one-hot encoding categorical variables, and inspecting transformed data to ensure quality for machine learning tasks.','OkCupid Dataset','en',_binary '\0'),(22,'Comparing Model Performance: Linear Regression, Decision Tree, and Random Forest for Predicting Mating Success','<p>In this blog post, we\'ll compare the performance of three regression models—linear Regression, Decision Tree Regression, and Random Forest Regression—applied to a dating dataset where we predict mating success.<br/><br/>\n\nWe will assess each model\'s performance based on Root Mean Squared Error (RMSE) and the results from cross-validation.</p>\n\n<h2>Linear Regression: Simple and Effective</h2>\n\n<p>Linear Regression is often the go-to model for regression tasks due to its simplicity and efficiency.<br/><br/>\n\n./chapter2/data2/train/train1.py</p>\n\n<pre><code class=\"language-python\">import os\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\n# Define the path to the dataset\nDATING_PATH = \"./chapter2/datasets/dating/copies5\"\n\n# Function to load the raw data\ndef load_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"6_classified_features_needed_label_stratified_train_set.csv\")\n    return pd.read_csv(csv_path)\n\n# Function to load the prepared (cleaned) data\ndef load_cleaned_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"6_1_cleaned_dating_data_features_and_labels.csv\")\n    return pd.read_csv(csv_path)\n\n# Load the raw data\ndating_data = load_dating_data()\n# print(f\"Rows after loading raw data: {dating_data.shape[0]}\")\n\n# Load the cleaned data (prepared data)\ndating_data_prepared = load_cleaned_dating_data()\n# print(f\"Rows after loading cleaned data: {dating_data_prepared.shape[0]}\")\n\n# Separate features and labels\nfeatures = dating_data.drop(\'mating_success\', axis=1)  # Remove the target variable from features\nlabels = dating_data[\'mating_success\']  # The target variable\n# print(f\"Rows after separating features and labels (raw data): {features.shape[0]}, {labels.shape[0]}\")\n\n# Separate features and labels for prepared (cleaned) data\nfeatures_prepared = dating_data_prepared.drop(\'mating_success\', axis=1)  # Remove the target variable from features\n# print(f\"Rows after separating features and labels (prepared data): {features_prepared.shape[0]}\")\n\n# Drop the persistent ID column(s) -- assuming the column name is \'persistent_id\' or something similar\nfeatures = features.drop(columns=[\'persistent_id\'], errors=\'ignore\')  # \'errors=ignore\' ensures no error if the column doesn\'t exist\nfeatures_prepared = features_prepared.drop(columns=[\'persistent_id\'], errors=\'ignore\')  # \'errors=ignore\' ensures no error if the column doesn\'t exist\n# print(f\"Rows after dropping \'persistent_id\' columns (raw and prepared): {features.shape[0]}, {features_prepared.shape[0]}\")\n\n# Initialize the Linear Regression model\nlin_reg = LinearRegression()\n\n# Train the model on the entire cleaned and scaled data\nlin_reg.fit(features_prepared, labels)\n# print(f\"Model trained with {features_prepared.shape[0]} rows.\")\n\n# Example: Make predictions on the first 5 rows of data\nsome_data = features.iloc[:5]  # First 5 data points\nsome_data_prepared = features_prepared.iloc[:5]  # First 5 data points\nsome_labels = labels.iloc[:5]  # First 5 labels\n\n# Make predictions\npredictions = lin_reg.predict(some_data_prepared)\n\n# Output predictions and actual labels\nprint(\"Predictions:\", predictions)\nprint(\"Labels:\", list(some_labels))\n\n# Calculate the Mean Squared Error and RMSE for the whole dataset\npredictions_all = lin_reg.predict(features_prepared)\nmse = mean_squared_error(labels, predictions_all)\nrmse = np.sqrt(mse)\n\nprint(\"RMSE:\", rmse)\n\n\n# Predictions: [0.2398392  1.08243193 2.84553505 0.5059264  0.99843327]\n# Labels: [0.0, 0.0, 4.0, 0.0, 0.0]\n# RMSE: 1.1439084247515616\n</code></pre>\n\n\n\n<p>Linear Regression yielded an RMSE of 1.1439 on the training set, suggesting that it effectively captured the main patterns in the data.</p>\n\n\n<h2>Decision Tree Regressor: Prone to Overfitting</h2>\n\n<p>The Decision Tree Regression Model is a powerful model, but it is highly prone to overfitting when trained on the entire dataset without regularization.<br/><br/>\n\n./chapter2/data2/train/train3.py</p>\n\n<pre><code class=\"language-python\">import os\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\n# Define the path to the dataset\nDATING_PATH = \"./chapter2/datasets/dating/copies5\"\n\n# Function to load the raw data\ndef load_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"6_classified_features_needed_label_stratified_train_set.csv\")\n    return pd.read_csv(csv_path)\n\n# Function to load the prepared (cleaned) data\ndef load_cleaned_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"6_1_cleaned_dating_data_features_and_labels.csv\")\n    return pd.read_csv(csv_path)\n\n# Load the raw data\ndating_data = load_dating_data()\n# print(f\"Rows after loading raw data: {dating_data.shape[0]}\")\n\n# Load the cleaned data (prepared data)\ndating_data_prepared = load_cleaned_dating_data()\n# print(f\"Rows after loading cleaned data: {dating_data_prepared.shape[0]}\")\n\n# Separate features and labels\nfeatures = dating_data.drop(\'mating_success\', axis=1)  # Remove the target variable from features\nlabels = dating_data[\'mating_success\']  # The target variable\n# print(f\"Rows after separating features and labels (raw data): {features.shape[0]}, {labels.shape[0]}\")\n\n# Separate features and labels for prepared (cleaned) data\nfeatures_prepared = dating_data_prepared.drop(\'mating_success\', axis=1)  # Remove the target variable from features\n# print(f\"Rows after separating features and labels (prepared data): {features_prepared.shape[0]}\")\n\n# Drop the persistent ID column(s) -- assuming the column name is \'persistent_id\' or something similar\nfeatures = features.drop(columns=[\'persistent_id\'], errors=\'ignore\')  # \'errors=ignore\' ensures no error if the column doesn\'t exist\nfeatures_prepared = features_prepared.drop(columns=[\'persistent_id\'], errors=\'ignore\')  # \'errors=ignore\' ensures no error if the column doesn\'t exist\n# print(f\"Rows after dropping \'persistent_id\' columns (raw and prepared): {features.shape[0]}, {features_prepared.shape[0]}\")\n\n# Initialize the Decision Tree Regressor model\ntree_reg = DecisionTreeRegressor()\n\n# Train the model on the entire cleaned and scaled data\ntree_reg.fit(features_prepared, labels)\n# print(f\"Model trained with {features_prepared.shape[0]} rows.\")\n\n# Example: Make predictions on the first 5 rows of data\nsome_data_prepared = features_prepared.iloc[:5]  # First 5 data points\nsome_labels = labels.iloc[:5]  # First 5 labels\n\n# Make predictions\npredictions = tree_reg.predict(some_data_prepared)\n\n# Output predictions and actual labels\nprint(\"Predictions:\", predictions)\nprint(\"Labels:\", list(some_labels))\n\n# Calculate the Mean Squared Error and RMSE for the whole dataset\npredictions_all = tree_reg.predict(features_prepared)\ntree_mse = mean_squared_error(labels, predictions_all)\ntree_rmse = np.sqrt(tree_mse)\n\nprint(\"Tree RMSE:\", tree_rmse)\n\n\n# Predictions: [0.  0.  4.  0.5 0. ]\n# Labels: [0.0, 0.0, 4.0, 0.0, 0.0]\n# Tree RMSE: 0.19546897919468237</code></pre>\n\n\n<p>The Decision Tree produced an RMSE of 0.1955 on the training data, indicating a very tight fit to the training set. However, we suspect overfitting, which is later confirmed during cross-validation.</p>\n\n\n<h2>Cross-Validation Summary</h2>\n\n<p>Cross-validation was used to check the generalization of the models:<br/><br/>\n\n./chapter2/data2/train/train4.py</p>\n\n<pre><code class=\"language-python\">import os\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\n# Define the path to the dataset\nDATING_PATH = \"./chapter2/datasets/dating/copies5\"\n\n# Function to load the raw data\ndef load_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"6_classified_features_needed_label_stratified_train_set.csv\")\n    return pd.read_csv(csv_path)\n\n# Function to load the prepared (cleaned) data\ndef load_cleaned_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"6_1_cleaned_dating_data_features_and_labels.csv\")\n    return pd.read_csv(csv_path)\n\n# Load the raw data\ndating_data = load_dating_data()\n# print(f\"Rows after loading raw data: {dating_data.shape[0]}\")\n\n# Load the cleaned data (prepared data)\ndating_data_prepared = load_cleaned_dating_data()\n# print(f\"Rows after loading cleaned data: {dating_data_prepared.shape[0]}\")\n\n# Separate features and labels\nfeatures = dating_data.drop(\'mating_success\', axis=1)  # Remove the target variable from features\nlabels = dating_data[\'mating_success\']  # The target variable\n\n# Separate features and labels for prepared (cleaned) data\nfeatures_prepared = dating_data_prepared.drop(\'mating_success\', axis=1)  # Remove the target variable from features\n\n# Drop the persistent ID column(s) -- assuming the column name is \'persistent_id\' or something similar\nfeatures = features.drop(columns=[\'persistent_id\'], errors=\'ignore\')  # \'errors=ignore\' ensures no error if the column doesn\'t exist\nfeatures_prepared = features_prepared.drop(columns=[\'persistent_id\'], errors=\'ignore\')  # \'errors=ignore\' ensures no error if the column doesn\'t exist\n\n# Initialize the models\ntree_reg = DecisionTreeRegressor()\nlin_reg = LinearRegression()\nforest_reg = RandomForestRegressor()\n\n# Function to display RMSE scores\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n    print()\n\n# 1. Decision Tree Cross-validation\ntree_scores = cross_val_score(tree_reg, features_prepared, labels, \n                              scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-tree_scores)\nprint(\"Decision Tree Regressor:\")\ndisplay_scores(tree_rmse_scores)\n\n# 2. Linear Regression Cross-validation\nlin_scores = cross_val_score(lin_reg, features_prepared, labels, \n                              scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\nprint(\"Linear Regression:\")\ndisplay_scores(lin_rmse_scores)\n\n# 3. Random Forest Regressor Cross-validation\nforest_scores = cross_val_score(forest_reg, features_prepared, labels, \n                                 scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\nprint(\"Random Forest Regressor:\")\ndisplay_scores(forest_rmse_scores)\n\n\n\n# Decision Tree Regressor:\n# Scores: [1.61247778 1.61731719 1.52758062 1.62473912 1.62029613 1.60026824\n#  1.72091084 1.56723718 1.61000336 1.65374219]\n# Mean: 1.6154572652677406\n# Standard deviation: 0.047975024747069335\n\n# Linear Regression:\n# Scores: [1.087067   1.20642808 1.14716698 1.14047653 1.17885481 1.19030231\n#  1.20752076 1.1213884  1.05076323 1.13498424]\n# Mean: 1.1464952332536744\n# Standard deviation: 0.048762821595965296\n\n# Random Forest Regressor:\n# Scores: [1.1325505  1.27366403 1.19187283 1.17668492 1.20896404 1.23495211\n#  1.27242249 1.17953747 1.11942813 1.21552121]\n# Mean: 1.2005597735833196\n# Standard deviation: 0.049275556706436206</code></pre>\n\n\n\n<h2>Conclusion</h2>\n\n<p>We also ran a Random Forest Regressor using cross-validation to assess its performance.<br/><br/>\n\nThe Random Forest model showed a mean RMSE of 1.2006 and a standard deviation 0.0493. While it performed better than the decision tree, its cross-validation performance could have been better than linear Regression.</p>\n\n<ul>\n<li>Linear Regression: Best regarding RMSE and stability across folds.</li>\n<li>Decision Tree: It initially appeared to perform well but overfitted the data, as confirmed by poor cross-validation results.</li>\n<li>Random Forest: Better than decision tree, but less stable and accurate than linear Regression.</li>\n</ul>\n\n<p>In conclusion, linear Regression is the best model overall, with Random Forest being a solid alternative. The decision tree overfitted the data and is only recommended for this problem with additional regularization.</p>\n\n\n<h2>Links to other parts of the project</h2>\n\n<ul>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/okcupid-dataset-analysis-for-machine-learning\">OkCupid Dataset Analysis for Machine Learning</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/exploring-okcupid-dataset-with-machine-learning-predicting-mating-success-for-straight-males\">Exploring OkCupid Dataset with Machine Learning: Predicting Mating Success for Straight Males</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/exploring-okcupid-dataset-with-stratified-sampling\">Exploring OkCupid Dataset with Stratified Sampling</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/discover-and-visualize-the-data-to-gain-insights-on-feature-vs-label-correlations-okcupid-dataset\">Discover and Visualize the Data to Gain Insights on Feature vs Label Correlations: OkCupid Dataset</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/okcupid-dataset-preparing-the-data-before-learning-algorithms\">OkCupid Dataset: Preparing the Data Before Learning Algorithms</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/comparing-model-performance-on-training-data-vs-test-data\">Comparing Model Performance on Training Data vs. Test Data</a></li>\n</ul>\n\n\n<h2>Annexe</h2>\n\n<p>Files produced from code executions:</p>\n\n<img src=\'/img/blog22/1.webp\' alt=\'Files produced from code executions\' target=\'_blank\' width=\'700\' height=\'auto\'/>\n\n','2024-11-20 18:00:40','2024-11-23 18:00:40','comparing-model-performance-linear-regression-decision-tree-and-random-forest-for-predicting-mating-success',6,'Learn how to compare the performance of Linear Regression, Decision Tree, and Random Forest models on a dating dataset. Discover key insights on RMSE, overfitting, and cross-validation results to choose the best model for predicting mating success.','OkCupid Dataset','en',_binary '\0'),(23,'Comparing Model Performance on Training Data vs. Test Data','<p>In this step of our machine learning project, we\'ve successfully completed several key steps in preparing our model for deployment:\nTraining on the training dataset\nPerforming cross-validation\nSelecting the best model (in our case, a linear regression model)<br/><br/>\n\nAfter these steps, we\'ve arrived at the critical phase of testing the model\'s performance on unseen data. This article will examine how the model performs on the training data versus the test data.</p>\n\n<h2>A Quick Recap: Preprocessing the Test Data</h2>\n\n<p>Before running our trained model on the test dataset, several preprocessing steps were applied to ensure the data was in the right format for prediction. Below are the key transformations and actions we performed, which were essential for preparing the data:</p>\n\n\n<ul>\n<li> Raw Data Edits: The raw test data was initially loaded and cleaned by removing missing values. In particular, we dealt with the mating_success column (our target variable) by removing NA values, ensuring that our data was consistent and usable for model testing.</li>\n<li> Cleaned the test Data: We cleaned the test data to produce the preprocessed test data.</li>\n<li> Saved the trained model based on the training data.</li>\n<li> ID Removal: The test data was filtered to remove entries with persistent_ids not present in the preprocessed test data (which was cleaned).</li>\n<li> Entry verification and consistency: We also ensured that the features in the test dataset matched those in the preprocessed test dataset. In this case, we verified that the features in the test set aligned with those in the cleaned training data (dating_data_prepared), ensuring consistency across datasets.</li>\n<li> Ensured all entries were labelled on cleaned and non-cleaned test data.</li>\n</ul>\n\n\n<p>These IDs serve as unique identifiers and do not contain any predictive value, so they are dropped to avoid introducing noise into the model.<br/><br/>\n\nOnce these preprocessing steps were completed, the model was ready to be tested on the unseen data (the test dataset). This process ensures the model performs under conditions similar to real-world applications, where it encounters new, previously unseen data.</p>\n\n<h2>Test the model against the test set</h2>\n\n<p>Feature and Label Separation: After cleaning, we separated the test data and preprocessed it into features (the input variables) and labels (the target variable, mating_success). This is a crucial step, as the model uses only the features to make predictions while the labels evaluate the model\'s performance.<br/><br/>\n\nWe run the model to make predictions, compare them to the actual labels, and calculate the RMSE.</p>\n\n\n\n\n<pre><code class=\"language-python\">import os\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nimport joblib\nimport pandas as pd\n\n# Load the trained model from the file\nmodel_filename = \"./chapter2/data2/test/dating_model.pkl\"\nlin_reg = joblib.load(model_filename)\n\nprint(f\"Model loaded from {model_filename}\")\n\n\n# Define the path to the dataset\nDATING_PATH = \"./chapter2/datasets/dating/copies6\"\n\n# Function to load the raw data\ndef load_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"7_classified_features_needed_label_stratified_test_set.csv_filtered.csv\")\n    return pd.read_csv(csv_path)\n\n# Function to load the prepared (cleaned) data\ndef load_cleaned_dating_data(dating_path=DATING_PATH):\n    csv_path = os.path.join(dating_path, \"7_cleaned_classified_features_needed_label_stratified_test_set.csv\")\n    return pd.read_csv(csv_path)\n\n# Load the raw data\ndating_data = load_dating_data()\nprint(f\"Rows after loading raw data: {dating_data.shape[0]}\")\n\n# Load the cleaned data (prepared data)\ndating_data_prepared = load_cleaned_dating_data()\nprint(f\"Rows after loading cleaned data: {dating_data_prepared.shape[0]}\")\n\n# Separate features and labels\nfeatures = dating_data.drop(\'mating_success\', axis=1)  # Remove the target variable from features\nlabels = dating_data[\'mating_success\']  # The target variable\nprint(f\"\\n\\nRows after separating features and labels (raw data): {features.shape[0]}, {labels.shape[0]}\")\n\n# Separate features and labels for prepared (cleaned) data\nfeatures_prepared = dating_data_prepared.drop(\'mating_success\', axis=1)  # Remove the target variable from features\nprint(f\"Rows after separating features and labels (prepared data): {features_prepared.shape[0]}\")\n\n# Drop the persistent ID column(s) -- assuming the column name is \'persistent_id\' or something similar\nfeatures = features.drop(columns=[\'persistent_id\'], errors=\'ignore\')  # \'errors=ignore\' ensures no error if the column doesn\'t exist\nfeatures_prepared = features_prepared.drop(columns=[\'persistent_id\'], errors=\'ignore\')  # \'errors=ignore\' ensures no error if the column doesn\'t exist\nprint(f\"\\n\\nRows after dropping \'persistent_id\' columns (raw and prepared): {features.shape[0]}, {features_prepared.shape[0]}\")\n\n\n\n# Example: Make predictions on the first 5 rows of data\nsome_data = features.iloc[:5]  # First 5 data points\nsome_data_prepared = features_prepared.iloc[:5]  # First 5 data points\nsome_labels = labels.iloc[:5]  # First 5 labels\n\n# Make predictions\npredictions = lin_reg.predict(some_data_prepared)\n\n# Output predictions and actual labels\nprint(\"Predictions:\", predictions)\nprint(\"Labels:\", list(some_labels))\n\n# Calculate the Mean Squared Error and RMSE for the whole dataset\npredictions_all = lin_reg.predict(features_prepared)\nmse = mean_squared_error(labels, predictions_all)\nrmse = np.sqrt(mse)\n\nprint(\"RMSE:\", rmse)</code></pre>\n\n<h2>Model Evaluation: Training Data vs. Test Data</h2>\n\n<p>Now that our trained model is ready and the test dataset preprocessed let\'s evaluate how well the model performs on both the training and test data. We\'ll focus on two key metrics: Predictions and Root Mean Squared Error (RMSE).</p>\n\n<h3>Performance on the Training Data</h3>\n\n<p>First, look at the model\'s performance on the training data. After training the linear regression model, we tested it on the first five rows of the prepared training data (features_prepared). The output showed the following predictions and corresponding actual labels:</p>\n\n<h4>Predictions on Training Data:</h4>\n\n<pre><code class=\"language-python\">Predictions: [0.2398392  1.08243193 2.84553505 0.5059264  0.99843327]  \nLabels: [0.0, 0.0, 4.0, 0.0, 0.0]</code></pre>\n\n<h4>RMSE on Training Data:</h4>\n\n<pre><code class=\"language-python\">RMSE: 1.1439084247515616</code></pre>\n\nAs we can see from the predictions, the model can generally predict mating success values close to the actual labels. However, there is some error in the projections, particularly for higher values of the mating_success label, such as 4.0, which the model predicted as 2.8455.</p>\n\n<h3>Performance on the Test Data</h3>\n\n<p>Next, we tested the same model on the unseen test data. This is where the model\'s generalization ability is truly put to the test. We evaluated the model on the first five rows of the prepared test data (features_prepared), and the results were as follows:</p>\n\n<h4>Predictions on Test Data:</h4>\n\n<pre><code class=\"language-python\">Predictions: [0.41243245 0.56738681 1.55097905 1.20148879 0.1961678 ]  \nLabels: [0.0, 0.0, 4.0, 2.0, 2.0]</code></pre>\n\n<h4>RMSE on Test Data:</h4>\n\n<pre><code class=\"language-python\">RMSE: 1.1107366363418567</code></pre>\n\n<h3>Comparing Model Performance</h3>\n\n<p>Now that we have the predictions and RMSE values for both the training and test datasets let\'s compare the two:</p>\n\n<h4>Training Data:</h4>\n\n<p>The model\'s predictions on the training data were relatively close to the actual values. The RMSE for the training data was 1.1439, which is a reasonable value given the nature of the problem (a regression task predicting continuous values).</p>\n\n<h4>Test Data:</h4>\n\n<p>The RMSE slightly improved to 1.1107 on the test data. This indicates that the model performed better on the test set than the training data, suggesting that the model is generalizing well to new, unseen data.<br/><br/>\n\nThe key takeaway is that the RMSE values on the training and test datasets are close, which is a good sign. It suggests that the model does not overfit the training data—i.e., it can generalize well to unseen data, as indicated by the performance of the test dataset.</p>\n\n<h2>Conclusion</h2>\n\n<p>This project section explored how the trained linear regression model performed on the training and test datasets. After performing essential preprocessing steps on the test data (including cleaning, feature separation, and alignment), we evaluated the model\'s performance by comparing its predictions and RMSE values.<br/><br/>\n\nWhile the RMSE on both datasets is similar, the slight improvement in the test data suggests that the model generalizes well to new data. This is an encouraging result, as it indicates that the model has not overfitted to the training data and can likely make accurate predictions on new, unseen examples.</p>\n\n\n<h2>Links to other parts of the project</h2>\n\n<ul>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/okcupid-dataset-analysis-for-machine-learning\">OkCupid Dataset Analysis for Machine Learning</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/exploring-okcupid-dataset-with-machine-learning-predicting-mating-success-for-straight-males\">Exploring OkCupid Dataset with Machine Learning: Predicting Mating Success for Straight Males</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/exploring-okcupid-dataset-with-stratified-sampling\">Exploring OkCupid Dataset with Stratified Sampling</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/discover-and-visualize-the-data-to-gain-insights-on-feature-vs-label-correlations-okcupid-dataset\">Discover and Visualize the Data to Gain Insights on Feature vs Label Correlations: OkCupid Dataset</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/okcupid-dataset-preparing-the-data-before-learning-algorithms\">OkCupid Dataset: Preparing the Data Before Learning Algorithms</a></li>\n<li><a href=\"/blog/artificial-intelligence/blog-posting/comparing-model-performance-linear-regression-decision-tree-and-random-forest-for-predicting-mating-success\">Comparing Model Performance: Linear Regression, Decision Tree, and Random Forest for Predicting Mating Success</a></li>\n</ul>','2024-11-23 18:00:40','2024-11-23 18:00:40','comparing-model-performance-on-training-data-vs-test-data',6,'Learn how to export and import a trained machine learning model using Joblib in Python. This tutorial demonstrates how to save a trained linear regression model from one script and load it into another for making predictions, ensuring model reuse and easy deployment.','OkCupid Dataset','en',_binary '\0');
/*!40000 ALTER TABLE `blog_element_fr` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `blog_page_fr`
--

DROP TABLE IF EXISTS `blog_page_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `blog_page_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `h3_1` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `blog_page_fr`
--

LOCK TABLES `blog_page_fr` WRITE;
/*!40000 ALTER TABLE `blog_page_fr` DISABLE KEYS */;
INSERT INTO `blog_page_fr` VALUES (1,'Catégories/Category');
/*!40000 ALTER TABLE `blog_page_fr` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `business_data_fr`
--

DROP TABLE IF EXISTS `business_data_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `business_data_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `website_main_url` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `business_name` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `service_type` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `business_description` varchar(500) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `street_address` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `address_city` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `address_province_state` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `postal_code` varchar(10) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `address_country` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `telephone` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `latitude` double DEFAULT NULL,
  `longitude` double DEFAULT NULL,
  `telephone2` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `email` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `calendly_link` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `business_data_fr`
--

LOCK TABLES `business_data_fr` WRITE;
/*!40000 ALTER TABLE `business_data_fr` DISABLE KEYS */;
INSERT INTO `business_data_fr` VALUES (1,'https://opom.ca','Opom Canada','Blogging and Web Publisher','We publish hight quality blog articles in french and english','#### Example St #20','Gatineau','QC','### ###','Canada','+1 (123) 123 1234',43.6532,-79.3832,'###@@@!!!!','##@outlook.com','##');
/*!40000 ALTER TABLE `business_data_fr` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `category_fr`
--

DROP TABLE IF EXISTS `category_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `category_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `category_name` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,
  `slug` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,
  `category_description` text CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `category_fr`
--

LOCK TABLES `category_fr` WRITE;
/*!40000 ALTER TABLE `category_fr` DISABLE KEYS */;
INSERT INTO `category_fr` VALUES (1,'Construction and renovation','construction-and-renovation','##'),(3,'Construction et renovation','construction-et-renovation','##'),(4,'Law','law','##'),(5,'Money','money','##'),(6,'Artificial Intelligence','artificial-intelligence','##');
/*!40000 ALTER TABLE `category_fr` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `category_page_fr`
--

DROP TABLE IF EXISTS `category_page_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `category_page_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `under_h1` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `category_page_fr`
--

LOCK TABLES `category_page_fr` WRITE;
/*!40000 ALTER TABLE `category_page_fr` DISABLE KEYS */;
INSERT INTO `category_page_fr` VALUES (1,'Opom Canada');
/*!40000 ALTER TABLE `category_page_fr` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `contact_fr`
--

DROP TABLE IF EXISTS `contact_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `contact_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `placeholder_name` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `valid_format` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `placeholder_message` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `button` varchar(12) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `h2_1` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `contact_fr`
--

LOCK TABLES `contact_fr` WRITE;
/*!40000 ALTER TABLE `contact_fr` DISABLE KEYS */;
INSERT INTO `contact_fr` VALUES (1,'Nom','formats valides :','Le message est ici. Veuillez fournir des détails sur ce qui doit être fait et nous vous répondrons dans les plus brefs délais.','Envoyer','Contactez-nous');
/*!40000 ALTER TABLE `contact_fr` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `extra_service_page_fr`
--

DROP TABLE IF EXISTS `extra_service_page_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `extra_service_page_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `title` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,
  `html_content` text CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  `slug` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `last_modified` varchar(26) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `description` text CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  `under_h1` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=13 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `extra_service_page_fr`
--

LOCK TABLES `extra_service_page_fr` WRITE;
/*!40000 ALTER TABLE `extra_service_page_fr` DISABLE KEYS */;
INSERT INTO `extra_service_page_fr` VALUES (1,'Opom','<h2>All Citations and Listings towards https://opom.ca</h2>\n<div>\n  <a href=\"##\">Opom on</a><br>\n\n</div>\n\n<h2>All Social Media towards https://opom.ca</h2>\n<div>\n  <a href=\"##\">Opom on</a><br>\n</div>\n\n<h2>All Backlinks from blogs and other user websites towards https://opom.ca</h2>\n<div>\n  <a href=\"##\">Opom on</a><br>\n</div>','opom','2024-08-25T13:24:03.206Z','##','Identificateur de Liens');
/*!40000 ALTER TABLE `extra_service_page_fr` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `faq_fr`
--

DROP TABLE IF EXISTS `faq_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `faq_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `question` varchar(528) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `answer` text CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=9 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `faq_fr`
--

LOCK TABLES `faq_fr` WRITE;
/*!40000 ALTER TABLE `faq_fr` DISABLE KEYS */;
INSERT INTO `faq_fr` VALUES (1,'Quelle est la durée de vie de l\'asphalte ?','La durée de vie de l\'asphalte dépend de plusieurs facteurs, tels que la qualité de l\'installation, les conditions climatiques, et l\'entretien régulier. En général, une allée en asphalte bien entretenue peut durer entre 15 et 20 ans.'),(2,'Quelle est la différence entre le pavage et l\'asphalte ?','Le pavage peut se référer à l\'installation de différentes surfaces, comme les pavés en béton, les pavés unis, ou l\'asphalte. L\'asphalte, quant à lui, est un matériau spécifique utilisé pour recouvrir les routes, les allées, et les stationnements. Le pavage peut inclure l\'utilisation d\'asphalte ou d\'autres matériaux comme les pavés imbriqués.'),(3,'Quand est-il préférable de poser de l\'asphalte ?','La meilleure période pour poser de l\'asphalte est pendant les mois chauds et secs, généralement entre le printemps et l\'automne. Une température plus chaude permet à l\'asphalte de bien se compacter et de durcir correctement, assurant une installation plus durable.'),(4,'Comment entretenir une allée en béton ?','Pour entretenir une allée en béton, il est recommandé de la nettoyer régulièrement avec un tuyau d\'arrosage ou un nettoyeur haute pression pour enlever la saleté et les débris. De plus, appliquer un scellant tous les 2 à 3 ans peut aider à protéger le béton contre les fissures et les taches.'),(5,'Quelle est la différence entre le béton et l\'asphalte pour les allées ?','Le béton est plus durable et offre une plus longue durée de vie que l\'asphalte, mais il est aussi plus coûteux. L\'asphalte, en revanche, est moins cher et plus rapide à installer, mais il peut nécessiter plus d\'entretien à long terme.'),(6,'Peut-on installer du pavage en hiver ?','Il est déconseillé d\'installer du pavage en hiver en raison des températures froides qui peuvent affecter la qualité du travail. Le sol doit être correctement préparé et les matériaux comme l\'asphalte ou le béton nécessitent des conditions climatiques favorables pour durcir correctement.'),(7,'Comment savoir si mon pavage doit être remplacé ou simplement réparé ?','Si votre pavage présente de petites fissures ou des signes mineurs de dégradation, il peut généralement être réparé. Cependant, si vous constatez des fissures profondes, un affaissement ou des dommages étendus, il est peut-être temps de remplacer le pavage.'),(8,'Combien de temps faut-il pour que le béton sèche complètement ?','Le béton met généralement 24 à 48 heures pour sécher au point où il peut supporter du poids léger, comme la marche. Toutefois, pour un séchage complet et pour pouvoir supporter des véhicules, il est recommandé d\'attendre au moins 7 jours.');
/*!40000 ALTER TABLE `faq_fr` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `footer_fr`
--

DROP TABLE IF EXISTS `footer_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `footer_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `youtube_alt` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `twitter_alt` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `pinterest_alt` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `quora_alt` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `email_alt` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `sitemap` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `blog` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `legal_disclaimer` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `privacy_policy` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `sitemap_path` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `blog_path` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `legal_disclaimer_path` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `privacy_policy_path` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `powered` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `footer_fr`
--

LOCK TABLES `footer_fr` WRITE;
/*!40000 ALTER TABLE `footer_fr` DISABLE KEYS */;
INSERT INTO `footer_fr` VALUES (1,'Image SVG du logo Youtube','Image SVG du logo Twitter','Image SVG du logo Pinterest','Image SVG du logo Quora','Image SVG d\'une lettre','Plan du site','Blog','Mention légale','Politique de confidentialité','/plan-du-site','/blog','/tiroir1/mention-legale','/tiroir1/politique-de-confidentialite','Propulsé par :');
/*!40000 ALTER TABLE `footer_fr` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `index_content_fr`
--

DROP TABLE IF EXISTS `index_content_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `index_content_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `title` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `content` text CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  `img_alt` text CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  `img_path` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `sub_title` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `index_content_fr`
--

LOCK TABLES `index_content_fr` WRITE;
/*!40000 ALTER TABLE `index_content_fr` DISABLE KEYS */;
INSERT INTO `index_content_fr` VALUES (1,'Extra 1','Lorem, ipsum dolor sit amet consectetur adipisicing elit. Distinctio, molestias. Quisquam dignissimos unde non, ipsa veritatis porro a et nemo? Minima ipsam corporis in illum dolores, voluptas mollitia quibusdam reprehenderit! 1','##','/img/opom_large.webp','Lorem, ipsum dolor 1'),(2,'Extra 2','Lorem, ipsum dolor sit amet consectetur adipisicing elit. Distinctio, molestias. Quisquam dignissimos unde non, ipsa veritatis porro a et nemo? Minima ipsam corporis in illum dolores, voluptas mollitia quibusdam reprehenderit! 2','##','/img/opom_large.webp','Lorem, ipsum dolor 2'),(3,'Extra 3','Lorem, ipsum dolor sit amet consectetur adipisicing elit. Distinctio, molestias. Quisquam dignissimos unde non, ipsa veritatis porro a et nemo? Minima ipsam corporis in illum dolores, voluptas mollitia quibusdam reprehenderit! 3','##','/img/opom_large.webp','Lorem, ipsum dolor 3');
/*!40000 ALTER TABLE `index_content_fr` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `main_service_data_fr`
--

DROP TABLE IF EXISTS `main_service_data_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `main_service_data_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `service_type` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `service_name` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `service_image_url` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `service_page_url` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `service_sub_title` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `slug` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `service_description` text CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  `service_image_path` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `service_image_description` text CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  `last_modified` varchar(26) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `web_page_content` text COLLATE utf8mb4_unicode_ci,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `main_service_data_fr`
--

LOCK TABLES `main_service_data_fr` WRITE;
/*!40000 ALTER TABLE `main_service_data_fr` DISABLE KEYS */;
INSERT INTO `main_service_data_fr` VALUES (1,'Type1','Drywall Kingston','https://opom.ca/img/drywall_kingston.webp','https://opom.ca/seo/drywall-kingston','Identificateur de Liens','drywall-kingston','<p>###</p>','/img/drywall_kingston.webp','##','2024-08-24T18:06:01.856Z','\n\n<h2>All Citations and Listings towards https://drywallkingston.com</h2>\n<div>\n  <a href=\"https://www.homestars.com/companies/3009679-drywall-kingston-earnanswers\">Drywall Kingston on\n    HomeStars</a><br>\n  <a href=\"https://trustedpros.ca/company/drywall-kingston-earnanswers\">Drywall Kingston on TrustedPros</a><br>\n  <a href=\"https://www.alignable.com/kingston-on/drywall-kingston-earnanwers\">Drywall Kingston on Alignable</a><br>\n  <a href=\"https://www.n49.com/biz/5998740/drywall-kingston-earnanswers-on-kingston-1786-bath-rd-18\">Drywall Kingston\n    on n49</a><br>\n  <a\n    href=\"https://www.hotfrog.ca/company/e7a2e194f3746a0a19ac39d288b47272/drywall-kingston-earnanswers/kingston/contractor-equipment-services\">Drywall\n    Kingston on Hotfrog</a><br>\n  <a href=\"https://www.canadianplanet.net/drywall-kingston-F1109C60F1FD343\">Drywall Kingston on Canadian\n    Planet</a><br>\n  <a href=\"https://classified.bonghaat.com/drywall-kingston-earnanswers/5029\">Drywall Kingston on Bonghaat</a><br>\n  <a href=\"https://www.manta.com/ic/m1wdyw1/ca/drywall-kingston-earnanswers\">Drywall Kingston on Manta</a><br>\n  <a href=\"https://www.merchantcircle.com/drywall-kingston-earnanwers-north-pomfret-vt\">Drywall Kingston on\n    MerchantCircle</a><br>\n  <a href=\"https://ezlocal.com/vt/kingston/contractors---drywall/0918143669\">Drywall Kingston on EZlocal</a><br>\n  <!-- <a href=\"https://ebusinesspages.com/Drywall-Kingston-Earnanswers_enk4v.co?PostReturn=0\">Drywall Kingston on eBusinessPages</a><br> -->\n  <a\n    href=\"https://www.chamberofcommerce.com/business-directory/new-york/cape-vincent/dry-wall-contractor/2033196579-drywall-kingston-earnanswers\">Drywall\n    Kingston on Chamber of Commerce</a><br>\n  <a href=\"http://chittendencounty.bizlistusa.com/business/5417417.htm\">Drywall Kingston on BizListUSA</a><br>\n  <a href=\"https://dyrectory.com/listings/drywall-kingston-earnanswers\">Drywall Kingston on Dyrectory</a><br>\n  <a href=\"https://citysquares.com/b/drywall-kingston-earnanswers-25733295\">Drywall Kingston on CitySquares</a><br>\n  <a href=\"https://www.yelp.ca/biz/drywall-kingston-kingston\">Drywall Kingston on Yelp</a><br>\n  <a\n    href=\"https://ca.nextdoor.com/pages/drywall-kingston-earnanswers/?init_source=org_pages&utm_campaign=1718465899619\">Drywall\n    Kingston on NextDoor</a><br>\n  <a href=\"https://www.profilecanada.com/companydetail.cfm?company=310750_Drywall_Kingston_Earnanswers_Kingston_ON\">Drywall\n    Kingston on Profile Canada</a><br />\n  <a\n    href=\"https://classifieds.thewhig.com/kingston/business-card-directory/drywall-kingston-earnanswers/987a6bfe48ca4be3bb02abda492e\">Drywall\n    Kingston on the thewhig.com</a><br />\n\n  <!-- Added July 26 -->\n  <a href=\"https://new.biddingowl.com/AuctionLanding?QueryAuctionId=b5e7be7a-f4aa-4cc9-99bd-e4dcdd46b0bb\">Drywall\n    Kingston on BiddingOwl</a><br />\n  <a href=\"https://www.flokii.com/businesses/view/135759/drywall-kingston-earnanswers\">Drywall Kingston on\n    Flokii</a><br />\n  <a href=\"https://supportkingston.ca/?post_type=listing&p=13065\">Drywall Kingston on SupportKingston</a><br />\n\n</div>\n\n<h2>All Social Media towards https://drywallkingston.com</h2>\n<div>\n  <a href=\"https://www.facebook.com/profile.php?id=61560891117055\">Drywall Kingston on Facebook</a><br>\n</div>\n\n<h2>All Backlinks from blogs and other user websites towards https://drywallkingston.com</h2>\n<div>\n  <a href=\"https://deco-malin.fr/comprendre-les-differents-types-de-cloisons-seches/\">Drywall Kingston on\n    Deco-Malin</a><br />\n  <a\n    href=\"https://www.eorvan.com/post/le-guide-ultime-du-webmaster-ma%C3%AEtrisez-les-rouages-de-la-cr%C3%A9ation-de-sites-web\">Drywall\n    Kingston on Eorvan</a><br />\n  <a href=\"https://www.bagueenargent.com/produit/grosse-bague-argent-homme-2/\">Drywall Kingston on BagueEnArgent</a>\n</div>'),(2,'Type2','Earnanswers','https://opom.ca/img/earnanswers.webp','https://opom.ca/seo/earnanswers','Identificateur de Liens','earnanswers','<p>###</p>','/img/earnanswers.webp','##','2024-08-24T18:06:01.856Z','<h2>All Citations and Listings towards https://earnanswers.com</h2>\n<div>\n  <a\n    href=\"https://nextdoor.com/pages/website-agency-gatineau-earnanswers-ogdensburg-ny/?init_source=org_pages&utm_campaign=1719846280749\">Earnanswers\n    on Nextdoor</a><br />\n</div>\n\n\n<h2>All Backlinks from blogs and other user websites towards https://earnanswers.com</h2>\n<div>\n  <a href=\"https://digiseine.fr/creation-site-web-rouen/\">Earnanswers on Digiseine</a><br />\n  <a href=\"https://chicetnoeud.com/blogs/articles-de-blog/quel-noeud-papillon-pour-un-costume-bleu\">Earnanswers on\n    Chic et Noeud</a><br />\n  <a href=\"https://inspirefrance.fr/intraparis-nomade-compte-agents-paris/\">Earnanswers on Inspire France</a><br />\n  <a href=\"https://www.eorvan.com/post/l-importance-du-seo-pour-votre-site-web\">Earnanswers on Eorvan</a><br />\n  <a href=\"https://www.bagueenargent.com/produit/grosse-bague-argent-homme-2/\">Earnanswers on Bague en Argent</a>\n</div>\n'),(3,'Type3','Bidblock','https://opom.ca/img/bidblock.webp','https://opom.ca/seo/bidblock','Identificateur de Liens','bidblock','<p>###</p>','/img/bidblock.webp','##','2024-08-24T18:06:01.856Z','<h2>All Backlinks from blogs and other user websites towards https://bidblock.ca</h2>\n<div>\n	<a href=\"https://www.bagueenargent.com/produit/grosse-bague-argent-homme-2/\">Bidblock on Bague en Argent</a><br/>\n	<a href=\"https://www.eorvan.com/post/le-guide-ultime-du-webmaster-ma%C3%AEtrisez-les-rouages-de-la-cr%C3%A9ation-de-sites-web\">Bidblock on Eorvan</a><br/>\n</div>');
/*!40000 ALTER TABLE `main_service_data_fr` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `nav_fr`
--

DROP TABLE IF EXISTS `nav_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `nav_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `op1` varchar(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `op2` varchar(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `logo_img_alt` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `op1_link` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `nav_fr`
--

LOCK TABLES `nav_fr` WRITE;
/*!40000 ALTER TABLE `nav_fr` DISABLE KEYS */;
INSERT INTO `nav_fr` VALUES (1,'Contact','Réserver un appel','Image du logo de l\'organisation','/contact');
/*!40000 ALTER TABLE `nav_fr` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `plan_du_site_page_fr`
--

DROP TABLE IF EXISTS `plan_du_site_page_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `plan_du_site_page_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `page_title` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `under_h1` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `meta_description` varchar(528) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `h2_1` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `h3_1` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `h3_2` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `plan_du_site_page_fr`
--

LOCK TABLES `plan_du_site_page_fr` WRITE;
/*!40000 ALTER TABLE `plan_du_site_page_fr` DISABLE KEYS */;
INSERT INTO `plan_du_site_page_fr` VALUES (1,'Plan du Site','Opom Canada','##6','Plan du site','Plan du site HTML','Plan du site XML');
/*!40000 ALTER TABLE `plan_du_site_page_fr` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `portfolio_section_fr`
--

DROP TABLE IF EXISTS `portfolio_section_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `portfolio_section_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `op1` varchar(50) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `op2` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `portfolio_section_fr`
--

LOCK TABLES `portfolio_section_fr` WRITE;
/*!40000 ALTER TABLE `portfolio_section_fr` DISABLE KEYS */;
INSERT INTO `portfolio_section_fr` VALUES (1,'plus de référencement','Sites');
/*!40000 ALTER TABLE `portfolio_section_fr` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `review_data_fr`
--

DROP TABLE IF EXISTS `review_data_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `review_data_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `name` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `rating_value` double DEFAULT NULL,
  `review_body` text CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `review_data_fr`
--

LOCK TABLES `review_data_fr` WRITE;
/*!40000 ALTER TABLE `review_data_fr` DISABLE KEYS */;
INSERT INTO `review_data_fr` VALUES (1,'Jean-Pierre D.',5,'Excellent service du début à la fin! L\'équipe a pavé notre allée en un temps record, tout en maintenant une qualité irréprochable.'),(2,'Mélanie L.',5,'J\'ai fait appel à cette entreprise pour refaire l\'asphalte de mon entrée de garage. Non seulement le travail a été réalisé à la perfection, mais l\'équipe était également très professionnelle et respectueuse des délais. Le résultat est impeccable et l\'entrée a un superbe aspect neuf. Merci beaucoup!'),(3,'Louis M.',5,'Je suis extrêmement satisfait du travail effectué sur notre terrasse en béton. L\'équipe a su respecter notre budget tout en fournissant un résultat qui dépasse nos attentes. La finition est superbe et les délais ont été parfaitement respectés. Je n\'hésiterai pas à les recommander à mon entourage.');
/*!40000 ALTER TABLE `review_data_fr` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `welcome_section_fr`
--

DROP TABLE IF EXISTS `welcome_section_fr`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `welcome_section_fr` (
  `id` int NOT NULL AUTO_INCREMENT,
  `banner_img_alt` text CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  `op1` varchar(20) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `op2` varchar(30) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `banner_img_path` text CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci,
  `map_img_alt` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `welcome_section_fr`
--

LOCK TABLES `welcome_section_fr` WRITE;
/*!40000 ALTER TABLE `welcome_section_fr` DISABLE KEYS */;
INSERT INTO `welcome_section_fr` VALUES (1,'Trois matériaux de pavage différents côte à côte : des pavés en béton emboîtables dans des tons de gris, des ouvriers étalant de l\'asphalte lors d\'un projet de construction routière, et de grandes dalles en pierre rectangulaires avec une branche projetant une ombre sur la surface.','Adresse','À propos de nous','/img/pavage_gatineau_b1.webp','Carte montrant l\'emplacement de l\'entreprise locale de pavage, d\'asphalte et de béton résidentiel et commercial à Gatineau.');
/*!40000 ALTER TABLE `welcome_section_fr` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2025-01-20 11:35:09
